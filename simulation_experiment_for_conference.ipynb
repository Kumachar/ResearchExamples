{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T22:33:58.301460Z",
     "start_time": "2025-06-08T22:33:54.310146Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "import numpyro\n",
    "import numpyro.distributions as dist\n",
    "from numpyro.infer import MCMC, NUTS\n",
    "import arviz as az\n",
    "from scipy.special import gammaln, log1p, expit\n",
    "from pathlib import Path\n",
    "import re, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from scipy.stats     import gaussian_kde\n",
    "from scipy.integrate import trapezoid\n",
    "from scipy.interpolate import interp1d\n",
    "from typing import Dict, Any\n",
    "\n",
    "\n",
    "\n",
    "rng = np.random.default_rng(42)"
   ],
   "id": "3ad8323d3d4ed0a7",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Utilities for plotting and saving beta densities",
   "id": "1157f540f00bc57"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T22:33:58.569127Z",
     "start_time": "2025-06-08T22:33:58.542084Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save beta density histograms as JPEG files\n",
    "def save_beta_density_hist(\n",
    "        idata,\n",
    "        *,\n",
    "        output_folder: str | Path,\n",
    "        experiment: str,\n",
    "        regex: str = r\"beta_\\d+\",\n",
    "        bins: int = 40,\n",
    "        dpi: int = 300,\n",
    "):\n",
    "    \"\"\"\n",
    "    One figure per beta variable (KDE + histogram) written to\n",
    "    {output_folder}/{experiment}_{varname}.jpg.\n",
    "    Histogram now shows *frequency* and the x-axis label is removed.\n",
    "    \"\"\"\n",
    "    output_folder = Path(output_folder)\n",
    "    output_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    posterior = idata.posterior\n",
    "    var_names = [v for v in posterior.data_vars if re.fullmatch(regex, v)]\n",
    "    if not var_names:\n",
    "        raise ValueError(f\"No variables match pattern {regex!r}\")\n",
    "\n",
    "    for var in var_names:\n",
    "        draws = posterior[var].values.reshape(-1)\n",
    "\n",
    "        fig, (ax_kde, ax_hist) = plt.subplots(\n",
    "            2, 1,\n",
    "            figsize=(6, 4),\n",
    "            sharex=True,\n",
    "            gridspec_kw={\"height_ratios\": (2, 1)},\n",
    "        )\n",
    "\n",
    "        # KDE (top)\n",
    "        az.plot_kde(draws, ax=ax_kde, plot_kwargs={\"color\": \"steelblue\"})\n",
    "        ax_kde.set_ylabel(\"Density\")\n",
    "        ax_kde.set_title(\"\")  # no title\n",
    "        ax_kde.set_xlabel(\"\")  # remove x-axis label\n",
    "        ax_kde.grid(True)\n",
    "\n",
    "        # Histogram (bottom) — raw counts\n",
    "        ax_hist.hist(draws, bins=bins, density=False,\n",
    "                     color=\"steelblue\", alpha=0.6)\n",
    "        ax_hist.set_ylabel(\"Frequency\")\n",
    "        ax_hist.set_xlabel(\"\")  # remove x-axis label\n",
    "        ax_hist.grid(True)\n",
    "\n",
    "        fig.tight_layout()\n",
    "\n",
    "        fname = output_folder / f\"{experiment}_{var}.jpg\"\n",
    "        fig.savefig(fname, dpi=dpi, bbox_inches=\"tight\")\n",
    "        plt.close(fig)\n",
    "    print(f\"Saved experiment {experiment} beta densities to {output_folder}\")\n",
    "\n",
    "\n",
    "# Plot empirical prior vs posterior KDEs\n",
    "def plot_df_prior_vs_posterior(\n",
    "        df_sim,\n",
    "        idata,\n",
    "        *,\n",
    "        kde_bw: str | float | None = None,  # bandwidth for gaussian_kde\n",
    "        density_cut: float = 1e-3,  # threshold for x-axis limits\n",
    "        xgrid_len: int = 2000,\n",
    "        save_to: str | Path | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Overlay empirical prior (from profile-likelihoods) with posterior KDEs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_sim : DataFrame\n",
    "        Columns ['source', 'outcome', 'point', 'value'];  'value' is log-lik.\n",
    "    idata  : arviz.InferenceData\n",
    "        Must contain variables  beta_<s>.\n",
    "    kde_bw : str | float | None\n",
    "        Passed to gaussian_kde(bw_method=...).  None → Scott’s rule.\n",
    "    density_cut : float\n",
    "        x-axis is restricted to points where max(prior, posterior) > density_cut.\n",
    "    xgrid_len : int\n",
    "        Number of points used to draw the KDE curve.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from scipy.integrate import trapezoid\n",
    "    from scipy.stats import gaussian_kde\n",
    "\n",
    "    # ── 1) detect β_s in the posterior ───────────────────────────\n",
    "    beta_vars = [v for v in idata.posterior.data_vars\n",
    "                 if re.fullmatch(r\"beta_\\d+\", v)]\n",
    "    sources_avl = sorted(int(v.split(\"_\")[1]) for v in beta_vars)\n",
    "    if not sources_avl:\n",
    "        raise ValueError(\"No beta_* variables found in idata.posterior\")\n",
    "\n",
    "    # ── 2) prepare empirical priors ──────────────────────────────\n",
    "    prior_dens = {}\n",
    "    for s in sources_avl:\n",
    "        sub = df_sim[df_sim[\"source\"] == s]\n",
    "        g = sub.groupby(\"point\")[\"value\"].sum().sort_index()\n",
    "        x = g.index.to_numpy()\n",
    "        ll = g.values - g.values.max()\n",
    "        p = np.exp(ll)\n",
    "        p = p / trapezoid(p, x)  # normalise\n",
    "        prior_dens[s] = (x, p)\n",
    "\n",
    "    # ── 3) KDEs for posterior samples ────────────────────────────\n",
    "    post_kde = {}\n",
    "    sample_rng = {}\n",
    "    for s in sources_avl:\n",
    "        samples = idata.posterior[f\"beta_{s}\"].values.flatten()\n",
    "        post_kde[s] = gaussian_kde(samples, bw_method=kde_bw)\n",
    "        sample_rng[s] = (samples.min(), samples.max())\n",
    "\n",
    "    # ── 4) plotting ──────────────────────────────────────────────\n",
    "    n = len(sources_avl)\n",
    "    ncols, nrows = 2, int(np.ceil(n / 2))\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(10, 4 * nrows))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for idx, s in enumerate(sources_avl):\n",
    "        x_prior, p_prior = prior_dens[s]\n",
    "        xmin = min(x_prior.min(), sample_rng[s][0])\n",
    "        xmax = max(x_prior.max(), sample_rng[s][1])\n",
    "        xgrid = np.linspace(xmin, xmax, xgrid_len)\n",
    "\n",
    "        p_post = post_kde[s](xgrid)\n",
    "\n",
    "        # ---- x-axis trimming by density_cut ---------------------\n",
    "        mask = np.maximum(\n",
    "            np.interp(xgrid, x_prior, p_prior, left=0, right=0),\n",
    "            p_post\n",
    "        ) > density_cut\n",
    "\n",
    "        if mask.any():\n",
    "            x_lim = (xgrid[mask].min(), xgrid[mask].max())\n",
    "        else:  # fallback: full span\n",
    "            x_lim = (xmin, xmax)\n",
    "\n",
    "        # ---- draw curves ---------------------------------------\n",
    "        axes[idx].plot(x_prior, p_prior, label=\"Prior (Interpolated)\")\n",
    "        axes[idx].plot(xgrid, p_post, label=\"Posterior (KDE)\")\n",
    "\n",
    "        axes[idx].set_title(f\"Source {s}\")\n",
    "        axes[idx].set_xlabel(rf\"$\\beta_{{{s}}}$\")\n",
    "        axes[idx].set_ylabel(\"Density\")\n",
    "        axes[idx].grid(True)\n",
    "        axes[idx].legend()\n",
    "        axes[idx].set_xlim(*x_lim)\n",
    "\n",
    "    # hide unused axes\n",
    "    for ax in axes[n:]:\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    save_to\n",
    "    if save_to is not None:\n",
    "        fig.savefig(save_to, dpi=300, bbox_inches=\"tight\")\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# KL divergence between prior and posterior densities\n",
    "def kl_prior_posterior_beta(\n",
    "    df_sim,\n",
    "    idata,\n",
    "    *,\n",
    "    posterior_bins: int | None = None,\n",
    "    eps: float = 1e-12,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute KL( posterior || prior) for each beta_s parameter.\n",
    "\n",
    "    The prior density is taken from the profile-likelihood table `df_sim`\n",
    "    (already normalised to integrate to 1).\n",
    "    The posterior density is estimated with a histogram that uses exactly\n",
    "    the same support grid, so the numerical integration is straightforward.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_sim : DataFrame\n",
    "        Columns: ['source','outcome','point','value'] with log-likelihood\n",
    "        values per grid point.\n",
    "    idata : arviz.InferenceData\n",
    "        Must contain posterior variables  beta_<s>.\n",
    "    posterior_bins : int, optional\n",
    "        If given, overrides the grid from `df_sim` and builds an equally\n",
    "        spaced grid with this many bins.  `None` (default) means: “use the\n",
    "        grid contained in df_sim”.\n",
    "    eps : float\n",
    "        Small positive number added to both densities to avoid log(0).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    kl_dict : dict  {source: KL divergence}\n",
    "    \"\"\"\n",
    "    import re\n",
    "    import numpy as np\n",
    "    from scipy.integrate import trapezoid\n",
    "\n",
    "    # -------- 1) Which β_s parameters are present?\n",
    "    beta_vars   = [v for v in idata.posterior.data_vars if re.fullmatch(r\"beta_\\d+\", v)]\n",
    "    sources_avl = sorted(int(v.split(\"_\")[1]) for v in beta_vars)\n",
    "    if not sources_avl:\n",
    "        raise ValueError(\"No beta_* variables found in idata.posterior\")\n",
    "\n",
    "    kl_dict = {}\n",
    "\n",
    "    for s in sources_avl:\n",
    "        # ---------- prior density on its grid --------------------\n",
    "        sub = df_sim[df_sim[\"source\"] == s]\n",
    "        g   = sub.groupby(\"point\")[\"value\"].sum().sort_index()\n",
    "        x_prior = g.index.to_numpy()\n",
    "        ll = g.values - g.values.max()\n",
    "        p_prior = np.exp(ll)\n",
    "        p_prior = p_prior / trapezoid(p_prior, x_prior)\n",
    "\n",
    "        # ---------- choose the grid / bin edges ------------------\n",
    "        if posterior_bins is None:\n",
    "            # use the df_sim grid\n",
    "            # build edges so that each bin is centred on a prior grid point\n",
    "            dx           = np.diff(x_prior)\n",
    "            left_edges   = x_prior[:-1] - dx / 2.0\n",
    "            right_edges  = x_prior[:-1] + dx / 2.0\n",
    "            edges        = np.concatenate(\n",
    "                ([left_edges[0]], right_edges, [x_prior[-1] + dx[-1] / 2.0])\n",
    "            )\n",
    "        else:\n",
    "            edges = np.linspace(x_prior.min(), x_prior.max(), posterior_bins + 1)\n",
    "\n",
    "        # ---------- posterior density on the same grid -----------\n",
    "        samples = idata.posterior[f\"beta_{s}\"].values.flatten()\n",
    "        counts, _ = np.histogram(samples, bins=edges, density=True)\n",
    "        # `counts` is already a density (area = 1); align it with mid-points\n",
    "        if posterior_bins is None:\n",
    "            p_post = counts\n",
    "            dx = np.diff(edges)                # width per bin\n",
    "        else:\n",
    "            # re-evaluate prior on the new grid to match dimensions\n",
    "            mid   = 0.5 * (edges[:-1] + edges[1:])\n",
    "            from scipy.interpolate import interp1d\n",
    "            interp = interp1d(x_prior, p_prior, bounds_error=False, fill_value=0.0)\n",
    "            p_prior = interp(mid)\n",
    "            p_post  = counts\n",
    "            dx      = np.diff(edges)\n",
    "\n",
    "        # ---------- KL divergence (numerical integral) -----------\n",
    "        # add eps to avoid division by / log of zero\n",
    "        p_post_safe  = p_post  + eps\n",
    "        p_prior_safe = p_prior + eps\n",
    "        kl = np.sum(dx * p_post_safe * np.log(p_post_safe / p_prior_safe))\n",
    "\n",
    "        kl_dict[s] = kl\n",
    "\n",
    "    return kl_dict\n",
    "\n",
    "# Summarise prior and posterior statistics for beta parameters\n",
    "def summarise_beta_prior_posterior(\n",
    "        df_sim,\n",
    "        idata,\n",
    "        kl_vals,\n",
    "        experiment_label: int = 1\n",
    "):\n",
    "    \"\"\"\n",
    "    For each beta_s parameter:\n",
    "\n",
    "        • numerical mean  & median of the PRIOR density\n",
    "        • mean, median and 0.95 CI of the POSTERIOR draws\n",
    "        • flag indicating whether the CI contains the prior mean\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    stats_df : pandas.DataFrame\n",
    "        Columns\n",
    "            source\n",
    "            prior_mean   prior_median\n",
    "            post_mean    post_median\n",
    "            post_ci_lower  post_ci_upper\n",
    "            ci_covers_prior_mean   (bool)\n",
    "    \"\"\"\n",
    "\n",
    "    # 1 ── locate the beta variables in the posterior\n",
    "    beta_vars = [v for v in idata.posterior.data_vars if re.fullmatch(r\"beta_\\d+\", v)]\n",
    "\n",
    "    sources_avl = sorted(int(v.split(\"_\")[1]) for v in beta_vars)\n",
    "    if not sources_avl:\n",
    "        raise ValueError(\"No beta_* variables found in idata.posterior\")\n",
    "\n",
    "    records = []\n",
    "\n",
    "    # 2 ── loop over sources\n",
    "    for s in sources_avl:\n",
    "        # ----- PRIOR density on grid ----------------------------\n",
    "        sub = df_sim[df_sim[\"source\"] == s]\n",
    "        g = sub.groupby(\"point\")[\"value\"].sum().sort_index()\n",
    "        x = g.index.to_numpy()\n",
    "        ll = g.values - g.values.max()  # stabilise exponentiation\n",
    "        p = np.exp(ll)\n",
    "        p = p / trapezoid(p, x)  # normalise\n",
    "\n",
    "        prior_mean = trapezoid(x * p, x)\n",
    "\n",
    "        # median: CDF crosses 0.5\n",
    "        # cumulative integral (trapezoidal)\n",
    "        cdf = np.concatenate(\n",
    "            ([0.0],\n",
    "             np.cumsum((p[:-1] + p[1:]) / 2 * np.diff(x)))\n",
    "        )\n",
    "        # interpolate to find the 0.5 point\n",
    "        median_interp = interp1d(cdf, x, bounds_error=False)\n",
    "        prior_median = float(median_interp(0.5))\n",
    "\n",
    "        # ----- POSTERIOR summary --------------------------------\n",
    "        samples = idata.posterior[f\"beta_{s}\"].values.flatten()\n",
    "        post_mean = float(np.mean(samples))\n",
    "        post_median = float(np.median(samples))\n",
    "        ci_lower, ci_upper = np.percentile(samples, [2.5, 97.5])\n",
    "\n",
    "        covers = (ci_lower <= prior_mean) and (prior_mean <= ci_upper)\n",
    "        mean_error = prior_mean - post_mean\n",
    "        median_error = prior_median - post_median\n",
    "\n",
    "        records.append(dict(\n",
    "            experiment = experiment_label,\n",
    "            source=\"beta{}\".format(s),\n",
    "            prior_mean=prior_mean,\n",
    "            prior_median=prior_median,\n",
    "            post_mean=post_mean,\n",
    "            post_median=post_median,\n",
    "            mean_error=mean_error,\n",
    "            median_error=median_error,\n",
    "            kl_divergence = kl_vals.get(s, np.nan),\n",
    "            post_ci_lower=ci_lower,\n",
    "            post_ci_upper=ci_upper,\n",
    "            ci_covers_prior_mean=covers,\n",
    "        ))\n",
    "\n",
    "    stats_df = pd.DataFrame.from_records(records)\n",
    "    return stats_df"
   ],
   "id": "ebd1e41a33734b7f",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Utilities using sample",
   "id": "8f51527edadef5f8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T11:47:17.876478Z",
     "start_time": "2025-06-08T11:47:17.844994Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_df_prior_vs_posterior(\n",
    "    idata,\n",
    "    beta_df,\n",
    "    *,\n",
    "    kde_bw: str | float | None = None,   # gaussian_kde bandwidth\n",
    "    density_cut: float = 1e-3,           # trim x-axis where both dens < cut\n",
    "    xgrid_len: int = 2_000,\n",
    "    save_to: str | Path | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Overlay prior and posterior densities for every β_s found in *idata*.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    idata : arviz.InferenceData\n",
    "        Must contain variables  beta_<s>.\n",
    "    prior_samples : dict[int, ndarray]\n",
    "        Mapping  {source s → 1-D array of prior β draws}.\n",
    "    kde_bw, density_cut, xgrid_len, save_to\n",
    "        Same meaning as before.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from scipy.stats import gaussian_kde\n",
    "\n",
    "    # ── 1) locate β_s in the posterior ────────────────────────────────\n",
    "    prior_samples = (\n",
    "    beta_df\n",
    "    .groupby(\"source\")[\"beta_true\"]\n",
    "    .apply(lambda s: s.values)       # dict: {source → 1-D array}\n",
    "    .to_dict()\n",
    "    )\n",
    "    beta_vars   = [v for v in idata.posterior.data_vars\n",
    "                   if re.fullmatch(r\"beta_\\d+\", v)]\n",
    "    sources_avl = sorted(int(v.split(\"_\")[1]) for v in beta_vars)\n",
    "    if not sources_avl:\n",
    "        raise ValueError(\"No beta_* variables found in idata.posterior\")\n",
    "\n",
    "    # ── 2) KDEs for prior & posterior samples ─────────────────────────\n",
    "    prior_kde  = {}\n",
    "    post_kde   = {}\n",
    "    sample_rng = {}\n",
    "\n",
    "    for s in sources_avl:\n",
    "        if s not in prior_samples:\n",
    "            raise KeyError(f\"prior_samples missing key {s}\")\n",
    "\n",
    "        # prior KDE\n",
    "        prior_draws = np.asarray(prior_samples[s]).ravel()\n",
    "        prior_kde[s] = gaussian_kde(prior_draws, bw_method=kde_bw)\n",
    "\n",
    "        # posterior KDE\n",
    "        post_draws  = idata.posterior[f\"beta_{s}\"].values.ravel()\n",
    "        post_kde[s] = gaussian_kde(post_draws,  bw_method=kde_bw)\n",
    "\n",
    "        sample_rng[s] = (min(prior_draws.min(), post_draws.min()),\n",
    "                         max(prior_draws.max(), post_draws.max()))\n",
    "\n",
    "    # ── 3) plotting ───────────────────────────────────────────────────\n",
    "    n      = len(sources_avl)\n",
    "    ncols  = 2\n",
    "    nrows  = int(np.ceil(n / ncols))\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(10, 4 * nrows))\n",
    "    axes   = axes.flatten()\n",
    "\n",
    "    for idx, s in enumerate(sources_avl):\n",
    "        xmin, xmax = sample_rng[s]\n",
    "        xgrid      = np.linspace(xmin, xmax, xgrid_len)\n",
    "\n",
    "        p_prior = prior_kde[s](xgrid)\n",
    "        p_post  = post_kde[s](xgrid)\n",
    "\n",
    "        # ---- x-axis trimming by density_cut --------------------------\n",
    "        mask = np.maximum(p_prior, p_post) > density_cut\n",
    "        if mask.any():\n",
    "            axes[idx].set_xlim(xgrid[mask].min(), xgrid[mask].max())\n",
    "\n",
    "        # ---- draw curves --------------------------------------------\n",
    "        axes[idx].plot(xgrid, p_prior, label=\"Prior (KDE)\")\n",
    "        axes[idx].plot(xgrid, p_post,  label=\"Posterior (KDE)\")\n",
    "\n",
    "        axes[idx].set_title(f\"Source {s}\")\n",
    "        axes[idx].set_xlabel(rf\"$\\beta_{{{s}}}$\")\n",
    "        axes[idx].set_ylabel(\"Density\")\n",
    "        axes[idx].grid(True)\n",
    "        axes[idx].legend()\n",
    "\n",
    "    for ax in axes[n:]:\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    if save_to is not None:\n",
    "        fig.savefig(save_to, dpi=300, bbox_inches=\"tight\")\n",
    "        plt.close(fig)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "def kl_prior_posterior_beta(\n",
    "    idata,\n",
    "    beta_df: pd.DataFrame | None = None,  # prior samples\n",
    "    *,\n",
    "    posterior_bins: int = 200,    # size of a *shared* equally–spaced grid\n",
    "    eps: float = 1e-12,\n",
    "):\n",
    "    \"\"\"\n",
    "    KL(  posterior  ||  prior )   for each beta_s, where **both** densities are\n",
    "    estimated from Monte-Carlo draws (prior draws you supply + posterior draws\n",
    "    inside *idata*).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    idata : arviz.InferenceData\n",
    "        Must contain posterior variables named  beta_<s>.\n",
    "    prior_samples : dict {source -> 1-D array of draws}\n",
    "        The raw β draws representing the prior for every source.\n",
    "        Keys must match the integers “s” in beta_<s>.\n",
    "    posterior_bins : int\n",
    "        Number of equally spaced histogram bins used for *both* densities.\n",
    "    eps : float\n",
    "        Tiny value added to each density to avoid log(0).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    kl_dict : dict {source : KL divergence (float)}\n",
    "    \"\"\"\n",
    "    prior_samples = (\n",
    "    beta_df\n",
    "    .groupby(\"source\")[\"beta_true\"]\n",
    "    .apply(lambda s: s.values)\n",
    "    .to_dict()\n",
    "    )\n",
    "\n",
    "    # 1) which β_s exist in the posterior?\n",
    "    beta_vars   = [v for v in idata.posterior.data_vars if re.fullmatch(r\"beta_\\d+\", v)]\n",
    "    sources_avl = sorted(int(v.split(\"_\")[1]) for v in beta_vars)\n",
    "    if not sources_avl:\n",
    "        raise ValueError(\"No beta_* variables found in idata.posterior\")\n",
    "\n",
    "    kl_dict = {}\n",
    "\n",
    "    for s in sources_avl:\n",
    "        if s not in prior_samples:\n",
    "            raise KeyError(f\"prior_samples missing key {s}\")\n",
    "\n",
    "        # ----- sample arrays --------------------------------------------\n",
    "        prior_draws = np.asarray(prior_samples[s]).ravel()\n",
    "        post_draws  = idata.posterior[f\"beta_{s}\"].values.ravel()\n",
    "\n",
    "        # ----- shared histogram grid ------------------------------------\n",
    "        xmin = min(prior_draws.min(), post_draws.min())\n",
    "        xmax = max(prior_draws.max(), post_draws.max())\n",
    "        edges = np.linspace(xmin, xmax, posterior_bins + 1)\n",
    "        widths = np.diff(edges)                # Δx per bin\n",
    "        mids   = 0.5 * (edges[:-1] + edges[1:])\n",
    "\n",
    "        # ----- densities (area = 1) -------------------------------------\n",
    "        prior_hist, _ = np.histogram(prior_draws, bins=edges, density=True)\n",
    "        post_hist,  _ = np.histogram(post_draws,  bins=edges, density=True)\n",
    "\n",
    "        # ----- KL integral  Σ p_post * log(p_post / p_prior) * Δx --------\n",
    "        p_post  = post_hist  + eps\n",
    "        p_prior = prior_hist + eps\n",
    "        kl      = np.sum(widths * p_post * np.log(p_post / p_prior))\n",
    "\n",
    "        kl_dict[s] = kl\n",
    "\n",
    "    return kl_dict\n",
    "\n",
    "def summarise_beta_prior_posterior(\n",
    "    idata,\n",
    "    beta_df: pd.DataFrame | None = None,\n",
    "    *,\n",
    "    kl_dict: dict[int, float] | None = None,\n",
    "    experiment_label: int | str = 1,\n",
    "):\n",
    "    \"\"\"\n",
    "    Summarise PRIOR vs POSTERIOR for each β_s.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    idata : arviz.InferenceData\n",
    "        Must contain posterior variables named  beta_<s>.\n",
    "    prior_samples : dict {source → 1-D array of draws}\n",
    "        Monte-Carlo draws representing the prior for every source.\n",
    "    kl_dict : dict {source → KL divergence}, optional\n",
    "        If you have already computed KL( post || prior ) you can pass it here\n",
    "        so the values are copied into the output table.  Missing keys get NaN.\n",
    "    experiment_label : hashable\n",
    "        Copied verbatim into the “experiment” column so you can concatenate\n",
    "        results from many runs.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    stats_df : pandas.DataFrame\n",
    "        Columns:\n",
    "            experiment  source\n",
    "            prior_mean  prior_median\n",
    "            post_mean   post_median\n",
    "            mean_error  median_error\n",
    "            kl_divergence\n",
    "            post_ci_lower  post_ci_upper\n",
    "            ci_covers_prior_mean   (bool)\n",
    "    \"\"\"\n",
    "    prior_samples = (\n",
    "    beta_df.groupby(\"source\")[\"beta_true\"]\n",
    "           .apply(lambda s: s.values)       # dict {source → draws}\n",
    "           .to_dict()\n",
    "    )\n",
    "\n",
    "    # ── 1) locate β_s in the posterior ──────────────────────────────────\n",
    "    beta_vars   = [v for v in idata.posterior.data_vars if re.fullmatch(r\"beta_\\d+\", v)]\n",
    "    sources_avl = sorted(int(v.split(\"_\")[1]) for v in beta_vars)\n",
    "    if not sources_avl:\n",
    "        raise ValueError(\"No beta_* variables found in idata.posterior\")\n",
    "\n",
    "    if kl_dict is None:\n",
    "        kl_dict = {}\n",
    "\n",
    "    records = []\n",
    "\n",
    "    # 2 ── per-source summary\n",
    "    for s in sources_avl:\n",
    "        if s not in prior_samples:\n",
    "            raise KeyError(f\"prior_samples missing key {s}\")\n",
    "\n",
    "        prior_draws  = np.asarray(prior_samples[s]).ravel()\n",
    "        prior_mean   = float(np.mean(prior_draws))\n",
    "        prior_median = float(np.median(prior_draws))\n",
    "\n",
    "        post_draws   = idata.posterior[f\"beta_{s}\"].values.ravel()\n",
    "        post_mean    = float(np.mean(post_draws))\n",
    "        post_median  = float(np.median(post_draws))\n",
    "        ci_lower, ci_upper = np.percentile(post_draws, [2.5, 97.5])   # ← call via np\n",
    "\n",
    "        records.append(dict(\n",
    "            experiment           = experiment_label,\n",
    "            source               = f\"beta{s}\",\n",
    "            prior_mean           = prior_mean,\n",
    "            prior_median         = prior_median,\n",
    "            post_mean            = post_mean,\n",
    "            post_median          = post_median,\n",
    "            mean_error           = prior_mean - post_mean,\n",
    "            median_error         = prior_median - post_median,\n",
    "            kl_divergence        = kl_dict.get(s, np.nan),\n",
    "            post_ci_lower        = ci_lower,\n",
    "            post_ci_upper        = ci_upper,\n",
    "            ci_covers_prior_mean = ci_lower <= prior_mean <= ci_upper,\n",
    "        ))\n",
    "\n",
    "    return pd.DataFrame.from_records(records)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "eee94099b5873213",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  experiment source  prior_mean  prior_median  post_mean  post_median  \\\n",
       "0       rep1  beta1    1.955559      1.954323   2.006640     2.000830   \n",
       "1       rep1  beta2    2.064780      2.064428   2.104604     2.111372   \n",
       "2       rep1  beta3    1.867465      1.876424   1.946909     1.938257   \n",
       "3       rep1  beta4    1.995572      1.978213   1.926708     1.923170   \n",
       "4       rep1  beta5    1.941855      1.955096   1.973095     1.961325   \n",
       "5       rep1  beta6    1.938002      1.934889   1.913469     1.916012   \n",
       "6       rep1  beta7    2.122200      2.106181   2.126976     2.124220   \n",
       "7       rep1  beta8    1.873554      1.867908   1.827984     1.832803   \n",
       "\n",
       "   mean_error  median_error  kl_divergence  post_ci_lower  post_ci_upper  \\\n",
       "0   -0.051081     -0.046507      26.840168       1.911594       2.121373   \n",
       "1   -0.039824     -0.046944      27.471347       2.020283       2.161230   \n",
       "2   -0.079444     -0.061833      27.608923       1.895246       2.039067   \n",
       "3    0.068864      0.055044      27.481497       1.877662       1.990490   \n",
       "4   -0.031240     -0.006229      26.841759       1.903849       2.096841   \n",
       "5    0.024532      0.018877      28.244644       1.850856       1.968141   \n",
       "6   -0.004777     -0.018040      26.510269       2.069363       2.196249   \n",
       "7    0.045569      0.035105      27.361624       1.713374       1.926672   \n",
       "\n",
       "   ci_covers_prior_mean  \n",
       "0                  True  \n",
       "1                  True  \n",
       "2                 False  \n",
       "3                 False  \n",
       "4                  True  \n",
       "5                  True  \n",
       "6                  True  \n",
       "7                  True  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment</th>\n",
       "      <th>source</th>\n",
       "      <th>prior_mean</th>\n",
       "      <th>prior_median</th>\n",
       "      <th>post_mean</th>\n",
       "      <th>post_median</th>\n",
       "      <th>mean_error</th>\n",
       "      <th>median_error</th>\n",
       "      <th>kl_divergence</th>\n",
       "      <th>post_ci_lower</th>\n",
       "      <th>post_ci_upper</th>\n",
       "      <th>ci_covers_prior_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rep1</td>\n",
       "      <td>beta1</td>\n",
       "      <td>1.955559</td>\n",
       "      <td>1.954323</td>\n",
       "      <td>2.006640</td>\n",
       "      <td>2.000830</td>\n",
       "      <td>-0.051081</td>\n",
       "      <td>-0.046507</td>\n",
       "      <td>26.840168</td>\n",
       "      <td>1.911594</td>\n",
       "      <td>2.121373</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rep1</td>\n",
       "      <td>beta2</td>\n",
       "      <td>2.064780</td>\n",
       "      <td>2.064428</td>\n",
       "      <td>2.104604</td>\n",
       "      <td>2.111372</td>\n",
       "      <td>-0.039824</td>\n",
       "      <td>-0.046944</td>\n",
       "      <td>27.471347</td>\n",
       "      <td>2.020283</td>\n",
       "      <td>2.161230</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rep1</td>\n",
       "      <td>beta3</td>\n",
       "      <td>1.867465</td>\n",
       "      <td>1.876424</td>\n",
       "      <td>1.946909</td>\n",
       "      <td>1.938257</td>\n",
       "      <td>-0.079444</td>\n",
       "      <td>-0.061833</td>\n",
       "      <td>27.608923</td>\n",
       "      <td>1.895246</td>\n",
       "      <td>2.039067</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rep1</td>\n",
       "      <td>beta4</td>\n",
       "      <td>1.995572</td>\n",
       "      <td>1.978213</td>\n",
       "      <td>1.926708</td>\n",
       "      <td>1.923170</td>\n",
       "      <td>0.068864</td>\n",
       "      <td>0.055044</td>\n",
       "      <td>27.481497</td>\n",
       "      <td>1.877662</td>\n",
       "      <td>1.990490</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rep1</td>\n",
       "      <td>beta5</td>\n",
       "      <td>1.941855</td>\n",
       "      <td>1.955096</td>\n",
       "      <td>1.973095</td>\n",
       "      <td>1.961325</td>\n",
       "      <td>-0.031240</td>\n",
       "      <td>-0.006229</td>\n",
       "      <td>26.841759</td>\n",
       "      <td>1.903849</td>\n",
       "      <td>2.096841</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rep1</td>\n",
       "      <td>beta6</td>\n",
       "      <td>1.938002</td>\n",
       "      <td>1.934889</td>\n",
       "      <td>1.913469</td>\n",
       "      <td>1.916012</td>\n",
       "      <td>0.024532</td>\n",
       "      <td>0.018877</td>\n",
       "      <td>28.244644</td>\n",
       "      <td>1.850856</td>\n",
       "      <td>1.968141</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>rep1</td>\n",
       "      <td>beta7</td>\n",
       "      <td>2.122200</td>\n",
       "      <td>2.106181</td>\n",
       "      <td>2.126976</td>\n",
       "      <td>2.124220</td>\n",
       "      <td>-0.004777</td>\n",
       "      <td>-0.018040</td>\n",
       "      <td>26.510269</td>\n",
       "      <td>2.069363</td>\n",
       "      <td>2.196249</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rep1</td>\n",
       "      <td>beta8</td>\n",
       "      <td>1.873554</td>\n",
       "      <td>1.867908</td>\n",
       "      <td>1.827984</td>\n",
       "      <td>1.832803</td>\n",
       "      <td>0.045569</td>\n",
       "      <td>0.035105</td>\n",
       "      <td>27.361624</td>\n",
       "      <td>1.713374</td>\n",
       "      <td>1.926672</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 178
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Profile Likelihood for Poisson Regression\n",
    "\n",
    "For each fixed value of $\\beta$ on grid, the log-likelihood for a single observation is\n",
    "\n",
    "$$\n",
    "\\ell_i(\\beta)\n",
    "= \\log p(y_i \\mid x_i, \\beta)\n",
    "= y_i \\,\\bigl(\\beta\\,x_i\\bigr)\n",
    "- \\exp\\!\\bigl(\\beta\\,x_i\\bigr)\n",
    "- \\log\\bigl(y_i!\\bigr).\n",
    "$$\n",
    "\n",
    "Since $\\beta$ is the only parameter, the **profile log-likelihood** is just the sum over all \\(n\\) observations:\n",
    "\n",
    "$$\n",
    "\\ell(\\beta)\n",
    "= \\sum_{i=1}^n \\ell_i(\\beta)\n",
    "= \\sum_{i=1}^n \\Bigl[y_i(\\beta\\,x_i) \\;-\\; e^{\\beta\\,x_i} \\;-\\; \\log(y_i!)\\Bigr].\n",
    "$$"
   ],
   "id": "25050883b8d08dd5"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-08T22:34:13.124001Z",
     "start_time": "2025-06-08T22:34:13.007325Z"
    }
   },
   "source": [
    "\n",
    "def simulate_profile_likelihoods_poisson(\n",
    "    K: int = 5,\n",
    "    S: int = 8,\n",
    "    O: int = 8,\n",
    "    n_obs: int = 100,\n",
    "    *,\n",
    "    beta_mean: np.ndarray | None = None,\n",
    "    beta_sds:  np.ndarray | None = None,\n",
    "    true_pis:  np.ndarray | None = None,\n",
    "    grid:      np.ndarray | None = None,\n",
    "    seed: int = 0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Simulate profile-likelihood curves for a K-component Poisson-regression mixture\n",
    "    on every (source, outcome) pair **and save the latent β actually used**.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_sim   : DataFrame\n",
    "        Columns ['source','outcome','point','value']  (log-lik values).\n",
    "    true_pis : ndarray, shape (S, K)\n",
    "        Mixture weights used for each source.\n",
    "    beta_df  : DataFrame\n",
    "        Columns ['source','outcome','beta_true']  — the latent β for each pair.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # ── 1) hyper-priors for the components ───────────────────────────────\n",
    "    if beta_mean is None:\n",
    "        beta_mean = np.linspace(-1, 1, K)\n",
    "    if beta_sds is None:\n",
    "        beta_sds = np.full(K, 0.15)\n",
    "\n",
    "    beta_mean = np.asarray(beta_mean, dtype=float)\n",
    "    beta_sds  = np.asarray(beta_sds,  dtype=float)\n",
    "    if beta_mean.shape != (K,) or beta_sds.shape != (K,):\n",
    "        raise ValueError(\"beta_mean and beta_sds must be length-K arrays\")\n",
    "\n",
    "    # ── 2) grid for profile likelihoods ──────────────────────────────────\n",
    "    if grid is None:\n",
    "        raise ValueError(\"Provide a 1-D array of grid points via `grid`\")\n",
    "    grid = np.asarray(grid, dtype=float)\n",
    "\n",
    "    # ── 3) Dirichlet mixture weights per source ──────────────────────────\n",
    "    if true_pis is None:\n",
    "        true_pis = rng.dirichlet(np.ones(K) * 2.0, size=S)  # shape (S, K)\n",
    "\n",
    "    # ── 4) simulation loop ───────────────────────────────────────────────\n",
    "    recs      = []        # for profile-likelihood rows\n",
    "    beta_recs = []        # for true β table\n",
    "\n",
    "    for s in range(S):                  # zero-based 0..S-1\n",
    "        pi_s = true_pis[s]\n",
    "\n",
    "        for o in range(O):              # zero-based 0..O-1\n",
    "            # 4-1) latent component for each observation\n",
    "            z      = rng.choice(K, size=n_obs, p=pi_s)\n",
    "            beta_i = rng.normal(loc=beta_mean[z], scale=beta_sds[z])\n",
    "\n",
    "            # save ONE β per (s,o): here we take the mean of individual β_i\n",
    "            beta_true = beta_i.mean()\n",
    "            beta_recs.append(dict(source=s + 1, outcome=o + 1, beta_true=beta_true))\n",
    "\n",
    "            # 4-2) x-values & Poisson responses\n",
    "            x_i = rng.uniform(-1, 1, size=n_obs)\n",
    "            lam = np.exp(beta_i * x_i)\n",
    "            y_i = rng.poisson(lam)\n",
    "\n",
    "            # 4-3) profile log-likelihood on grid\n",
    "            for b in grid:\n",
    "                logp = y_i * (b * x_i) - np.exp(b * x_i) - gammaln(y_i + 1)\n",
    "                recs.append(\n",
    "                    dict(source=s + 1, outcome=o + 1, point=b, value=logp.sum())\n",
    "                )\n",
    "\n",
    "    df_sim  = pd.DataFrame.from_records(recs)\n",
    "    beta_df = pd.DataFrame.from_records(beta_recs)\n",
    "\n",
    "    return df_sim, true_pis, beta_df\n",
    "\n",
    "grid_pts   = np.linspace(-10, 10, 100)\n",
    "#mean draw from gaussian with mean 1 and sd 0.15\n",
    "beta_mean = rng.normal(loc=2.0, scale=3.0, size=5)\n",
    "beta_sds = np.abs(rng.normal(loc=0.0, scale=1.0, size=5))\n",
    "\n",
    "df_sim, true_pis, beta_df = simulate_profile_likelihoods_poisson(\n",
    "    K=5, S=8, O=8, n_obs=100,\n",
    "    beta_mean=beta_mean,\n",
    "    beta_sds=beta_sds,\n",
    "    grid=grid_pts,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "df_sim.to_csv(\"simulated_profileLikelihoods.csv\", index=False)\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Profile Likelihood for Linear Regression\n",
    "\n",
    "For each outcome $o=1,\\dots,O$ and observation $i=1,\\dots,n$:\n",
    "\n",
    "\\begin{aligned}\n",
    "z_{s,o,i} &\\sim \\mathrm{Categorical}(\\pi_s),\\\\\n",
    "\\beta_{\\,z_{s,o,i}} &\\sim \\mathcal{N}(\\mu_{z_{s,o,i}},\\,\\sigma_{z_{s,o,i}}),\\\\\n",
    "x_{s,o,i} &\\sim \\mathrm{Uniform}(-1,1),\\\\\n",
    "y_{s,o,i} &= \\beta_{\\,z_{s,o,i}}\\,x_{s,o,i} + \\varepsilon_{s,o,i},\\quad\n",
    "\\varepsilon_{s,o,i}\\sim\\mathcal{N}(0,\\sigma^2).\n",
    "\\end{aligned}\n",
    "\n",
    "At each candidate $b$ in the grid:\n",
    "$$\n",
    "\\ell_{s,o}(b)\n",
    "=\n",
    "\\sum_{i=1}^n \\log \\bigl[\\mathcal{N}(y_{s,o,i}\\mid b\\,x_{s,o,i},\\,\\sigma^2)\\bigr]\n",
    "=\n",
    "-\\frac{n}{2}\\log(2\\pi\\sigma^2)\n",
    "\\;-\\;\n",
    "\\frac{1}{2\\sigma^2}\\sum_{i=1}^n\\bigl(y_{s,o,i}-b\\,x_{s,o,i}\\bigr)^2.\n",
    "$$"
   ],
   "id": "6b3cdb5766c2a7d2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T22:34:20.161281Z",
     "start_time": "2025-06-08T22:34:20.118099Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def simulate_profile_likelihoods_linreg(\n",
    "    K: int = 5,\n",
    "    S: int = 8,\n",
    "    O: int = 8,\n",
    "    n_obs: int = 100,\n",
    "    *,\n",
    "    beta_mean: np.ndarray | None = None,\n",
    "    beta_sds:  np.ndarray | None = None,\n",
    "    true_pis:  np.ndarray | None = None,\n",
    "    grid: np.ndarray | None = None,\n",
    "    sigma_noise: float = 1.0,\n",
    "    seed: int = 0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Linear-regression analogue to your Poisson simulator, now ALSO returning a\n",
    "    table of the latent β actually used for every (source, outcome) pair.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_sim   : DataFrame  ['source','outcome','point','value']  (log-lik).\n",
    "    true_pis : ndarray    shape (S, K)   – mixture weights.\n",
    "    beta_df  : DataFrame  ['source','outcome','beta_true']      – NEW.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # ── 1) component means / sds ──────────────────────────────────────────\n",
    "    if beta_mean is None:\n",
    "        beta_mean = np.linspace(-1, 1, K)\n",
    "    if beta_sds is None:\n",
    "        beta_sds = np.full(K, 0.15)\n",
    "    beta_mean = np.asarray(beta_mean, dtype=float)\n",
    "    beta_sds  = np.asarray(beta_sds,  dtype=float)\n",
    "    if beta_mean.shape != (K,) or beta_sds.shape != (K,):\n",
    "        raise ValueError(\"`beta_mean` and `beta_sds` must be length-K arrays\")\n",
    "\n",
    "    # ── 2) grid of candidate slopes ──────────────────────────────────────\n",
    "    if grid is None:\n",
    "        raise ValueError(\"Supply a 1-D array of grid points via `grid`.\")\n",
    "    grid = np.asarray(grid, dtype=float)\n",
    "\n",
    "    # ── 3) Dirichlet weights per source ──────────────────────────────────\n",
    "    if true_pis is None:\n",
    "        true_pis = rng.dirichlet(np.full(K, 2.0), size=S)   # (S, K)\n",
    "\n",
    "    # ── 4) simulate & profile-likelihood evaluation ─────────────────────\n",
    "    const_term = -0.5 * n_obs * np.log(2 * np.pi * sigma_noise**2)\n",
    "\n",
    "    recs      = []   # profile-likelihood rows\n",
    "    beta_recs = []   # true β per (s,o)\n",
    "\n",
    "    for s in range(S):                               # zero-based\n",
    "        pi_s = true_pis[s]\n",
    "\n",
    "        for o in range(O):                           # zero-based\n",
    "            # 4-1) latent component + β for each observation\n",
    "            z       = rng.choice(K, size=n_obs, p=pi_s)\n",
    "            beta_i  = rng.normal(loc=beta_mean[z], scale=beta_sds[z])\n",
    "\n",
    "            # save ONE representative β  (mean of draws works fine)\n",
    "            beta_recs.append(\n",
    "                dict(source=s + 1, outcome=o + 1, beta_true=beta_i.mean())\n",
    "            )\n",
    "\n",
    "            # 4-2) covariate & response\n",
    "            x_i = rng.uniform(-1, 1, size=n_obs)\n",
    "            y_i = beta_i * x_i + rng.normal(0.0, sigma_noise, size=n_obs)\n",
    "\n",
    "            # 4-3) profile log-likelihood over the grid\n",
    "            for b in grid:\n",
    "                resid_sq = (y_i - b * x_i) ** 2\n",
    "                ll       = const_term - 0.5 * resid_sq.sum() / sigma_noise**2\n",
    "                recs.append(\n",
    "                    dict(source=s + 1, outcome=o + 1, point=b, value=ll)\n",
    "                )\n",
    "\n",
    "    df_sim  = pd.DataFrame.from_records(recs)\n",
    "    beta_df = pd.DataFrame.from_records(beta_recs)\n",
    "\n",
    "    return df_sim, true_pis, beta_df\n",
    "\n",
    "grid_pts   = np.linspace(-10, 10, 100)\n",
    "beta_mean  = rng.normal(loc=1.5, scale=0.7, size=5)\n",
    "beta_sds = np.abs(rng.normal(loc=0.0, scale=1.0, size=5))\n",
    "\n",
    "df_sim, pis, beta_df = simulate_profile_likelihoods_linreg(\n",
    "    K=5, S=8, O=8, n_obs=120,\n",
    "    beta_mean=beta_mean,\n",
    "    beta_sds=beta_sds,\n",
    "    grid=grid_pts,\n",
    "    sigma_noise=0.8,\n",
    "    seed=2025\n",
    ")\n"
   ],
   "id": "384d0feadcbebd96",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Profile Likelihood for Binomial Regression\n",
    "\n",
    "For each outcome $o = 1,\\dots,O$ and observation $i = 1,\\dots,n$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "z_{s,o,i} &\\sim \\mathrm{Categorical}(\\pi_s),\\\\\n",
    "\\mu_{z_{s,o,i}} &\\sim \\mathcal{N}(2,9) ,\\\\\n",
    "\\sigma_{z_{s,o,i}} &\\sim \\mathrm{HalfNorm}(10.0),\\\\\n",
    "b_{z_{s,o,i}} &\\sim \\mathcal{N}(\\mu_{z_{s,o,i}},\\,\\sigma_{z_{s,o,i}}),\\\\\n",
    "x_{s,o,i} &\\sim \\mathrm{Uniform}(-1,1),\\\\\n",
    "p_{s,o,i} &= \\sigma\\!\\bigl(\\beta_{z_{s,o,i}}\\,x_{s,o,i}\\bigr),\n",
    "\\qquad\n",
    "\\sigma(u)=\\frac{1}{1+e^{-u}},\\\\\n",
    "y_{s,o,i} &\\sim \\mathrm{Binomial}\\!\\bigl(n_{s,o,i},\\,p_{s,o,i}\\bigr).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $n_{s,o,i}$ is the number of trials (often $n_{s,o,i}=1$ for Bernoulli data).\n",
    "\n",
    "---\n",
    "\n",
    "### Profile log-likelihood at candidate slope $b$\n",
    "\n",
    "$$\n",
    "\\ell_{s,o}(b)\n",
    "=\n",
    "\\sum_{i=1}^n\n",
    "\\log\\!\\Bigl[\n",
    "\\mathrm{Binomial}\\!\\bigl(\n",
    "y_{s,o,i}\\mid n_{s,o,i},\\,\\sigma(b_{s,o,i}\\,x_{s,o,i})\n",
    "\\bigr)\n",
    "\\Bigr]\n",
    "=\n",
    "\\sum_{i=1}^n\n",
    "\\left\\{\n",
    "y_{s,o,i}\\,\\log\\sigma(b_{s,o,i}\\,x_{s,o,i})\n",
    "+\\bigl(n_{s,o,i}-y_{s,o,i}\\bigr)\\,\\log\\!\\bigl[1-\\sigma(b_{s,o,i}\\,x_{s,o,i})\\bigr]\n",
    "+\\log\\binom{n_{s,o,i}}{y_{s,o,i}}\n",
    "\\right\\}.\n",
    "$$\n",
    "\n",
    "(The combinatorial term $\\log\\binom{n_{s,o,i}}{y_{s,o,i}}$ is constant in $b$ and can be omitted when profiling.)\n",
    "\n"
   ],
   "id": "9aecf2d58cbaf8e9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T22:34:27.770332Z",
     "start_time": "2025-06-08T22:34:27.700539Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def simulate_profile_likelihoods_logistic(\n",
    "    K: int = 5,\n",
    "    S: int = 8,\n",
    "    O: int = 8,\n",
    "    n_obs: int = 100,\n",
    "    *,\n",
    "    beta_mean: np.ndarray | None = None,\n",
    "    beta_sds:  np.ndarray | None = None,\n",
    "    true_pis:  np.ndarray | None = None,\n",
    "    grid:      np.ndarray | None = None,\n",
    "    seed: int = 0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Simulate profile-likelihood curves for a K-component *logistic-regression*\n",
    "    mixture **and keep the ground-truth β for every (source, outcome).**\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_sim   : DataFrame   ['source','outcome','point','value']  (log-liks)\n",
    "    true_pis : ndarray     shape (S, K)  – mixture weights used\n",
    "    beta_df  : DataFrame   ['source','outcome','beta_true']      – NEW\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # ── 1) component means / sds ─────────────────────────────────────────\n",
    "    if beta_mean is None:\n",
    "        beta_mean = np.linspace(-1, 1, K)\n",
    "    if beta_sds is None:\n",
    "        beta_sds = np.full(K, 0.15)\n",
    "    beta_mean = np.asarray(beta_mean, dtype=float)\n",
    "    beta_sds  = np.asarray(beta_sds,  dtype=float)\n",
    "    if beta_mean.shape != (K,) or beta_sds.shape != (K,):\n",
    "        raise ValueError(\"beta_mean and beta_sds must be length-K arrays\")\n",
    "\n",
    "    # ── 2) grid of candidate β values ───────────────────────────────────\n",
    "    if grid is None:\n",
    "        raise ValueError(\"Provide a 1-D array of grid points via `grid`\")\n",
    "    grid = np.asarray(grid, dtype=float)\n",
    "\n",
    "    # ── 3) Dirichlet weights per source ─────────────────────────────────\n",
    "    if true_pis is None:\n",
    "        true_pis = rng.dirichlet(np.ones(K) * 2.0, size=S)    # (S, K)\n",
    "\n",
    "    # ── 4) simulate & evaluate profile log-likelihoods ─────────────────\n",
    "    recs      = []   # profile-likelihood rows\n",
    "    beta_recs = []   # table of true β’s\n",
    "\n",
    "    for s in range(S):                              # zero-based 0..S-1\n",
    "        pi_s = true_pis[s]\n",
    "\n",
    "        for o in range(O):                          # zero-based 0..O-1\n",
    "            # 4-1) latent component & β draw per observation\n",
    "            z       = rng.choice(K, size=n_obs, p=pi_s)\n",
    "            beta_i  = rng.normal(loc=beta_mean[z], scale=beta_sds[z])\n",
    "\n",
    "            # save ONE representative β  (mean of draws works fine)\n",
    "            beta_recs.append(\n",
    "                dict(source=s + 1, outcome=o + 1, beta_true=beta_i.mean())\n",
    "            )\n",
    "\n",
    "            # 4-2) covariate & Bernoulli responses\n",
    "            x_i     = rng.uniform(-1, 1, size=n_obs)\n",
    "            logit_p = beta_i * x_i\n",
    "            p_i     = 1.0 / (1.0 + np.exp(-logit_p))\n",
    "            y_i     = rng.binomial(1, p_i)\n",
    "\n",
    "            # 4-3) profile log-likelihood over the grid\n",
    "            for b in grid:\n",
    "                bx   = b * x_i\n",
    "                ll   = (y_i * bx - np.log1p(np.exp(bx))).sum()\n",
    "                recs.append(\n",
    "                    dict(source=s + 1, outcome=o + 1, point=b, value=ll)\n",
    "                )\n",
    "\n",
    "    df_sim  = pd.DataFrame.from_records(recs)\n",
    "    beta_df = pd.DataFrame.from_records(beta_recs)\n",
    "    return df_sim, true_pis, beta_df\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "grid_pts  = np.linspace(-10, 10, 100)\n",
    "beta_mean = rng.normal(loc=2.0,  scale=3.0, size=5)\n",
    "beta_sds  = np.abs(rng.normal(loc=0.0, scale=1.0, size=5))\n",
    "\n",
    "df_sim, true_pis, beta_df = simulate_profile_likelihoods_logistic(\n",
    "    K=5, S=8, O=8, n_obs=100,\n",
    "    beta_mean=beta_mean,\n",
    "    beta_sds=beta_sds,\n",
    "    grid=grid_pts,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(df_sim.head())\n",
    "print(\"Mixture weights for each source:\\n\", true_pis)\n",
    "\n"
   ],
   "id": "26e9ebc29e83f6b0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   source  outcome      point       value\n",
      "0       1        1 -10.000000 -286.691449\n",
      "1       1        1  -9.797980 -281.255815\n",
      "2       1        1  -9.595960 -275.827702\n",
      "3       1        1  -9.393939 -270.407580\n",
      "4       1        1  -9.191919 -264.995952\n",
      "Mixture weights for each source:\n",
      " [[0.182076   0.2467942  0.15991495 0.14319025 0.2680246 ]\n",
      " [0.16226678 0.21711215 0.20262703 0.2848007  0.13319334]\n",
      " [0.35248377 0.10889546 0.22777007 0.2097213  0.10112939]\n",
      " [0.31192199 0.18316285 0.09769391 0.33921295 0.06800829]\n",
      " [0.08925351 0.17176974 0.28885723 0.25510165 0.19501787]\n",
      " [0.1023434  0.28595577 0.3380555  0.19739144 0.07625389]\n",
      " [0.19963775 0.2925956  0.13294668 0.32160435 0.05321562]\n",
      " [0.08485659 0.21323099 0.1347426  0.29983717 0.26733264]]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T20:48:09.402415Z",
     "start_time": "2025-06-08T20:48:09.230836Z"
    }
   },
   "cell_type": "code",
   "source": [
    "N_sources = 8\n",
    "source_outcome_data = {}\n",
    "for s in range(1, N_sources+1):\n",
    "    df_s = df_sim[df_sim[\"source\"]==s]\n",
    "    outcome_list = []\n",
    "    for o in sorted(df_s[\"outcome\"].unique()):\n",
    "        arr = df_s[df_s[\"outcome\"]==o][[\"point\",\"value\"]].to_numpy()\n",
    "        outcome_list.append(jnp.array(arr))\n",
    "    source_outcome_data[s] = outcome_list\n",
    "\n",
    "num_outcomes_dict = {s: len(source_outcome_data[s])\n",
    "                     for s in range(2, N_sources+1)}"
   ],
   "id": "d04d533a0af302fe",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## HDP Model",
   "id": "e5e7c99dadaf6fc3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T22:34:34.165094Z",
     "start_time": "2025-06-08T22:34:34.157836Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def stick_breaking(beta):\n",
    "    rem = jnp.concatenate([jnp.array([1.]), jnp.cumprod(1-beta)[:-1]])\n",
    "    return beta * rem\n",
    "\n",
    "def reparameterize(pi):\n",
    "    return pi / jnp.sum(pi)\n",
    "\n",
    "def custom_loglike(beta, outcome_data_list):\n",
    "    total_ll = 0.\n",
    "    for outcome_data in outcome_data_list:\n",
    "        x_vals = outcome_data[:,0]\n",
    "        loglike_vals = outcome_data[:,1]\n",
    "        total_ll += jnp.interp(beta, x_vals, loglike_vals)\n",
    "    return total_ll\n",
    "\n",
    "def HDP_model(source_outcome_data, num_outcomes_dict, N_sources, k, data_point_mean):\n",
    "    gamma = numpyro.sample(\"gamma\", dist.Gamma(1.0,5.0))\n",
    "    alpha0 = numpyro.sample(\"alpha0\", dist.Gamma(1.0,5.0))\n",
    "    beta_tilt = numpyro.sample(\"beta_tilt\", dist.Beta(1.,gamma).expand([k]))\n",
    "    beta = stick_breaking(beta_tilt)\n",
    "    beta_cum = jnp.cumsum(beta)\n",
    "    pi_tilt = numpyro.sample(\n",
    "        \"pi_tilt\",\n",
    "        dist.Beta(alpha0*beta, alpha0*(1-beta_cum))\n",
    "        .expand([N_sources,k])\n",
    "    )\n",
    "    pi_norms = []\n",
    "    for s in range(N_sources):\n",
    "        pi_s = stick_breaking(pi_tilt[s])\n",
    "        pi_norms.append(reparameterize(pi_s))\n",
    "    mu = numpyro.sample(\"mu\", dist.Normal(0,10.).expand([k]))\n",
    "    sigma = numpyro.sample(\"sigma\", dist.HalfNormal(10.).expand([k]))\n",
    "    for s in range(1, N_sources+1):\n",
    "        mix = dist.MixtureSameFamily(\n",
    "            dist.Categorical(probs=pi_norms[s-1]),\n",
    "            dist.Normal(loc=mu, scale=sigma)\n",
    "        )\n",
    "        beta_s = numpyro.sample(f\"beta_{s}\", mix)\n",
    "        grid_min = jnp.min(source_outcome_data[s][0][:,0])\n",
    "        grid_max = jnp.max(source_outcome_data[s][-1][:,0])\n",
    "        beta_clipped = jnp.clip(beta_s, grid_min, grid_max)\n",
    "        ll = custom_loglike(beta_clipped, source_outcome_data[s])\n",
    "        numpyro.factor(f\"loglike_{s}\", ll)"
   ],
   "id": "486843da367e7e58",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T22:34:48.581620Z",
     "start_time": "2025-06-08T22:34:48.573928Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpyro\n",
    "import numpyro.distributions as dist\n",
    "\n",
    "\n",
    "# --- keep your helper functions ------------------------------------------------\n",
    "def stick_breaking(beta):\n",
    "    portion_remaining = jnp.concatenate(\n",
    "        [jnp.array([1.]), jnp.cumprod(1. - beta)[:-1]]\n",
    "    )\n",
    "    return beta * portion_remaining\n",
    "\n",
    "\n",
    "def reparameterize(pi):\n",
    "    return pi / pi.sum()\n",
    "\n",
    "def custom_loglike(beta, outcome_data_list):\n",
    "    total_ll = 0.\n",
    "    for outcome_data in outcome_data_list:\n",
    "        x_vals = outcome_data[:,0]\n",
    "        loglike_vals = outcome_data[:,1]\n",
    "        total_ll += jnp.interp(beta, x_vals, loglike_vals)\n",
    "    return total_ll\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "def HDP_model(source_outcome_data,\n",
    "              num_outcomes_dict,\n",
    "              N_sources,\n",
    "              k,\n",
    "              data_point_mean):\n",
    "    gamma = numpyro.sample(\"gamma\", dist.Gamma(1.0, 5.0))\n",
    "    alpha0 = numpyro.sample(\"alpha0\", dist.Gamma(1.0, 5.0))\n",
    "\n",
    "    # ── global stick-breaking weights ──────────────────────────────────────────\n",
    "    beta_tilt = numpyro.sample(\"beta_tilt\", dist.Beta(1., gamma).expand([k]))\n",
    "    beta = stick_breaking(beta_tilt)\n",
    "    beta_cum = jnp.cumsum(beta)\n",
    "\n",
    "    # ── source–specific, *unnormalised* sticks ────────────────────────────────\n",
    "    pi_tilt = numpyro.sample(\n",
    "        \"pi_tilt\",\n",
    "        dist.Beta(alpha0 * beta, alpha0 * (1. - beta_cum)).expand([N_sources, k])\n",
    "    )  # shape (N_sources, k)\n",
    "\n",
    "    # ── compute *normalised* mixture weights and **record them** ──────────────\n",
    "    #     → appear as `pi_norm` in the trace / summary.\n",
    "    def _to_pi_norm(pi_row):\n",
    "        return reparameterize(stick_breaking(pi_row))  # (k,)\n",
    "\n",
    "    pi_norm = jax.vmap(_to_pi_norm)(pi_tilt)  # (N_sources, k)\n",
    "    numpyro.deterministic(\"pi_norm\", pi_norm)  # <<–– archived!\n",
    "\n",
    "    # ── remaining model parts (unchanged) ─────────────────────────────────────\n",
    "    mu = numpyro.sample(\"mu\", dist.Normal(0., 10.).expand([k]))\n",
    "    sigma = numpyro.sample(\"sigma\", dist.HalfNormal(10.).expand([k]))\n",
    "\n",
    "    for s in range(1, N_sources + 1):\n",
    "        mix = dist.MixtureSameFamily(\n",
    "            dist.Categorical(probs=pi_norm[s - 1]),\n",
    "            dist.Normal(loc=mu, scale=sigma),\n",
    "        )\n",
    "        beta_s = numpyro.sample(f\"beta_{s}\", mix)\n",
    "\n",
    "        grid_min = jnp.min(source_outcome_data[s][0][:, 0])\n",
    "        grid_max = jnp.max(source_outcome_data[s][-1][:, 0])\n",
    "        beta_clipped = jnp.clip(beta_s, grid_min, grid_max)\n",
    "\n",
    "        ll = custom_loglike(beta_clipped, source_outcome_data[s])\n",
    "        numpyro.factor(f\"loglike_{s}\", ll)"
   ],
   "id": "900a0bf2bb4f4413",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T21:15:39.378221Z",
     "start_time": "2025-06-08T21:14:06.611948Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 5) Run MCMC just as before\n",
    "rng_key = random.PRNGKey(0)\n",
    "data_point_mean = df_sim[\"point\"].mean()\n",
    "k = 5\n",
    "\n",
    "nuts = NUTS(HDP_model)\n",
    "mcmc = MCMC(nuts, num_warmup=5000, num_samples=20000)\n",
    "mcmc.run(\n",
    "    rng_key,\n",
    "    source_outcome_data=source_outcome_data,\n",
    "    num_outcomes_dict=num_outcomes_dict,\n",
    "    N_sources=N_sources,\n",
    "    k=k,\n",
    "    data_point_mean=data_point_mean\n",
    ")\n",
    "\n",
    "mcmc.print_summary()\n",
    "idata = az.from_numpyro(mcmc)"
   ],
   "id": "5aa5d10c91363c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|██████████| 25000/25000 [01:29<00:00, 280.01it/s, 30 steps of size 5.31e-02. acc. prob=0.37] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                  mean       std    median      5.0%     95.0%     n_eff     r_hat\n",
      "      alpha0      0.95      0.33      0.88      0.38      1.42    241.85      1.00\n",
      "      beta_1      0.13      0.12      0.13     -0.07      0.34    157.68      1.00\n",
      "      beta_2      1.31      0.13      1.31      1.08      1.52    310.35      1.00\n",
      "      beta_3      1.56      0.14      1.55      1.32      1.78    444.49      1.01\n",
      "      beta_4      1.59      0.14      1.59      1.35      1.84    268.66      1.00\n",
      "      beta_5      0.87      0.12      0.87      0.69      1.07    352.18      1.00\n",
      "      beta_6      1.12      0.13      1.11      0.90      1.32    177.72      1.00\n",
      "      beta_7      1.43      0.14      1.43      1.23      1.66    274.69      1.00\n",
      "      beta_8      0.58      0.12      0.58      0.38      0.77    161.69      1.02\n",
      "beta_tilt[0]      0.29      0.22      0.19      0.03      0.64     27.86      1.11\n",
      "beta_tilt[1]      0.29      0.21      0.21      0.03      0.63     49.79      1.05\n",
      "beta_tilt[2]      0.28      0.19      0.23      0.03      0.59     69.23      1.00\n",
      "beta_tilt[3]      0.28      0.16      0.25      0.03      0.51     84.63      1.03\n",
      "beta_tilt[4]      0.35      0.16      0.32      0.11      0.61    142.33      1.00\n",
      "       gamma      0.86      0.37      0.77      0.35      1.47    133.16      1.01\n",
      "       mu[0]      1.61      5.83      1.19     -6.65     12.97     76.95      1.00\n",
      "       mu[1]      0.85      9.30      1.09    -12.29     21.48    120.47      1.00\n",
      "       mu[2]      0.79      8.27      1.17    -13.86     14.48    275.32      1.00\n",
      "       mu[3]      0.36      9.72      0.68    -15.38     17.37    325.51      1.00\n",
      "       mu[4]     -0.85      9.43      0.03    -17.75     13.40    142.94      1.00\n",
      "pi_tilt[0,0]      0.30      0.37      0.07      0.00      0.95     44.32      1.06\n",
      "pi_tilt[0,1]      0.31      0.39      0.05      0.00      0.97     87.58      1.03\n",
      "pi_tilt[0,2]      0.27      0.39      0.00      0.00      0.99    239.36      1.01\n",
      "pi_tilt[0,3]      0.29      0.41      0.00      0.00      1.00    153.30      1.02\n",
      "pi_tilt[0,4]      0.29      0.41      0.00      0.00      1.00    179.47      1.00\n",
      "pi_tilt[1,0]      0.34      0.40      0.07      0.00      0.98     31.72      1.11\n",
      "pi_tilt[1,1]      0.30      0.40      0.02      0.00      0.99     71.30      1.03\n",
      "pi_tilt[1,2]      0.31      0.41      0.01      0.00      1.00    104.52      1.01\n",
      "pi_tilt[1,3]      0.24      0.38      0.00      0.00      0.99    423.04      1.00\n",
      "pi_tilt[1,4]      0.26      0.39      0.00      0.00      1.00    597.45      1.01\n",
      "pi_tilt[2,0]      0.32      0.38      0.07      0.00      0.96     40.27      1.07\n",
      "pi_tilt[2,1]      0.29      0.39      0.02      0.00      0.98     73.59      1.02\n",
      "pi_tilt[2,2]      0.30      0.40      0.01      0.00      0.99    107.32      1.01\n",
      "pi_tilt[2,3]      0.25      0.38      0.00      0.00      0.99    345.93      1.00\n",
      "pi_tilt[2,4]      0.29      0.41      0.00      0.00      1.00    527.36      1.00\n",
      "pi_tilt[3,0]      0.31      0.38      0.05      0.00      0.95     41.86      1.06\n",
      "pi_tilt[3,1]      0.29      0.39      0.02      0.00      0.98     95.69      1.03\n",
      "pi_tilt[3,2]      0.27      0.38      0.00      0.00      0.97    127.22      1.00\n",
      "pi_tilt[3,3]      0.26      0.39      0.00      0.00      0.99    159.18      1.01\n",
      "pi_tilt[3,4]      0.29      0.42      0.00      0.00      1.00    472.74      1.00\n",
      "pi_tilt[4,0]      0.33      0.39      0.07      0.00      0.97     47.14      1.08\n",
      "pi_tilt[4,1]      0.31      0.40      0.03      0.00      0.98    103.39      1.02\n",
      "pi_tilt[4,2]      0.24      0.38      0.00      0.00      0.98    195.57      1.00\n",
      "pi_tilt[4,3]      0.27      0.40      0.00      0.00      0.99    128.65      1.01\n",
      "pi_tilt[4,4]      0.29      0.41      0.00      0.00      1.00    273.04      1.00\n",
      "pi_tilt[5,0]      0.32      0.38      0.09      0.00      0.97     38.48      1.08\n",
      "pi_tilt[5,1]      0.29      0.38      0.02      0.00      0.97     83.32      1.03\n",
      "pi_tilt[5,2]      0.28      0.40      0.01      0.00      0.99     87.08      1.00\n",
      "pi_tilt[5,3]      0.26      0.39      0.00      0.00      0.99    234.79      1.01\n",
      "pi_tilt[5,4]      0.28      0.41      0.00      0.00      1.00    350.31      1.00\n",
      "pi_tilt[6,0]      0.33      0.39      0.08      0.00      0.97     32.76      1.12\n",
      "pi_tilt[6,1]      0.30      0.39      0.02      0.00      0.98     85.75      1.03\n",
      "pi_tilt[6,2]      0.26      0.38      0.00      0.00      0.98    123.85      1.00\n",
      "pi_tilt[6,3]      0.26      0.39      0.00      0.00      0.99    316.12      1.00\n",
      "pi_tilt[6,4]      0.27      0.39      0.00      0.00      1.00    741.04      1.01\n",
      "pi_tilt[7,0]      0.31      0.39      0.04      0.00      0.97     47.72      1.08\n",
      "pi_tilt[7,1]      0.30      0.40      0.01      0.00      0.99     77.19      1.03\n",
      "pi_tilt[7,2]      0.24      0.38      0.00      0.00      0.97    201.27      1.01\n",
      "pi_tilt[7,3]      0.26      0.39      0.00      0.00      0.99    195.41      1.00\n",
      "pi_tilt[7,4]      0.29      0.40      0.01      0.00      1.00    417.48      1.00\n",
      "    sigma[0]      4.51      5.44      1.56      0.06     12.16     53.10      1.12\n",
      "    sigma[1]      5.77      5.71      4.02      0.07     14.03    101.18      1.04\n",
      "    sigma[2]      6.14      5.87      4.51      0.00     14.64    163.31      1.00\n",
      "    sigma[3]      6.92      6.25      5.01      0.00     16.20    253.12      1.00\n",
      "    sigma[4]      6.77      6.03      5.29      0.00     15.69    217.29      1.00\n",
      "\n",
      "Number of divergences: 11603\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T22:35:03.002284Z",
     "start_time": "2025-06-08T22:35:02.987574Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def store_beta_posteriors(\n",
    "        idata,\n",
    "        df_sim: pd.DataFrame,\n",
    "        *,\n",
    "        output_folder: str | Path,\n",
    "        file_ext: str = \"npz\",\n",
    "        compress: bool = True,\n",
    "        regex: str = r\"beta_\\d+\",\n",
    ") -> dict[str, dict[str, np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Persist posterior samples of β parameters plus their empirical priors.\n",
    "\n",
    "    For each posterior variable that matches *regex*\n",
    "    (default pattern ``beta_<number>``) the function\n",
    "\n",
    "    1. extracts all posterior draws and flattens them to 1-D;\n",
    "    2. selects the slice of *df_sim* that belongs to the same *source*\n",
    "       (the integer following the underscore);\n",
    "    3. writes a single file\n",
    "\n",
    "           beta_<s>.npz   (or .npy)\n",
    "\n",
    "       with three NumPy arrays:\n",
    "           • samples – posterior draws            (shape = [n_draws])\n",
    "           • grid    – grid points used in prior  (shape = [m])\n",
    "           • loglik  – log-likelihood on that grid (shape = [m])\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    idata : arviz.InferenceData\n",
    "        Posterior container produced by NumPyro / ArviZ.\n",
    "    df_sim : pandas.DataFrame\n",
    "        Columns ['source','outcome','point','value']; *value* is a\n",
    "        log-likelihood.  Log-likelihoods for identical (source, point)\n",
    "        pairs are summed before being stored.\n",
    "    output_folder : str | pathlib.Path\n",
    "        Where the files are written (created if missing).\n",
    "    file_ext : {\"npz\",\"npy\"}, default \"npz\"\n",
    "        Storage format.  If \"npz\" you may enable compressed archives\n",
    "        via *compress=True* (default).\n",
    "    compress : bool, default True\n",
    "        Compression flag for the “npz” format.\n",
    "    regex : str, default r\"beta_\\\\d+\"\n",
    "        Pattern that selects β variables inside *idata.posterior*.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A mapping ``{ \"beta_<s>\" : { \"samples\":…, \"grid\":…, \"loglik\":… }, … }``\n",
    "        matching what has been written to disk.\n",
    "    \"\"\"\n",
    "\n",
    "    output_folder = Path(output_folder)\n",
    "    output_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    beta_vars = [v for v in idata.posterior.data_vars if re.fullmatch(regex, v)]\n",
    "    if not beta_vars:\n",
    "        raise ValueError(f\"No beta variables match {regex!r}\")\n",
    "\n",
    "    result: dict[str, dict[str, np.ndarray]] = {}\n",
    "\n",
    "    for var in sorted(beta_vars, key=lambda v: int(v.split(\"_\")[1])):\n",
    "        s = int(var.split(\"_\")[1])  # source number\n",
    "        samples = idata.posterior[var].values.reshape(-1)\n",
    "\n",
    "        # ---- empirical prior slice (summed over outcomes) ----\n",
    "        mask = df_sim[\"source\"] == s\n",
    "        if not mask.any():\n",
    "            raise ValueError(f\"df_sim contains no rows for source={s}\")\n",
    "\n",
    "        sub = (\n",
    "            df_sim.loc[mask, [\"point\", \"value\"]]\n",
    "            .groupby(\"point\", sort=True)[\"value\"]\n",
    "            .sum()\n",
    "            .sort_index()\n",
    "        )\n",
    "        grid, loglik = sub.index.to_numpy(), sub.values\n",
    "\n",
    "        bundle = {\"samples\": samples, \"grid\": grid, \"loglik\": loglik}\n",
    "        result[var] = bundle\n",
    "\n",
    "        fname = output_folder / f\"{var}.{file_ext}\"\n",
    "        if file_ext == \"npz\":\n",
    "            (np.savez_compressed if compress else np.savez)(fname, **bundle)\n",
    "        elif file_ext == \"npy\":\n",
    "            np.save(fname, bundle, allow_pickle=True)\n",
    "        else:\n",
    "            raise ValueError(\"file_ext must be 'npz' or 'npy'\")\n",
    "\n",
    "    print(f\"Stored {len(result)} β posteriors in {output_folder}\")\n",
    "    return result\n",
    "\n",
    "def load_beta_posteriors(\n",
    "        folder: str | Path,\n",
    "        *,\n",
    "        file_ext: str = \"npz\",\n",
    "        regex: str = r\"beta_\\d+\\.\"  # automatically completed with file_ext\n",
    ") -> dict[str, dict[str, np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Read back the objects produced by `store_beta_posteriors`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    folder : str | pathlib.Path\n",
    "        Directory that contains the saved `beta_<s>.<ext>` files.\n",
    "    file_ext : {\"npz\", \"npy\"}, default \"npz\"\n",
    "        Extension that was used when saving.\n",
    "    regex : str, default r\"beta_\\\\d+\\\\.\"\n",
    "        A prefix for the file-name pattern; the extension is appended\n",
    "        automatically.  Change only if you saved under a different name\n",
    "        convention.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Mapping ``{ \"beta_<s>\" : { \"samples\":…, \"grid\":…, \"loglik\":… }, … }``.\n",
    "    \"\"\"\n",
    "    folder = Path(folder)\n",
    "    if not folder.is_dir():\n",
    "        raise FileNotFoundError(f\"{folder} is not a directory\")\n",
    "\n",
    "    pattern = re.compile(regex + re.escape(file_ext) + r\"$\")\n",
    "    output: dict[str, dict[str, np.ndarray]] = {}\n",
    "\n",
    "    for f in sorted(folder.iterdir()):\n",
    "        if not pattern.fullmatch(f.name):\n",
    "            continue\n",
    "        varname = f.stem  # e.g. \"beta_1\"\n",
    "        if file_ext == \"npz\":\n",
    "            with np.load(f, allow_pickle=False) as data:\n",
    "                output[varname] = {\n",
    "                    \"samples\": data[\"samples\"],\n",
    "                    \"grid\": data[\"grid\"],\n",
    "                    \"loglik\": data[\"loglik\"],\n",
    "                }\n",
    "        elif file_ext == \"npy\":\n",
    "            output[varname] = np.load(f, allow_pickle=True).item()\n",
    "        else:\n",
    "            raise ValueError(\"file_ext must be 'npz' or 'npy'\")\n",
    "\n",
    "    if not output:\n",
    "        raise RuntimeError(f\"No *.{file_ext} files that match {regex} found in {folder}\")\n",
    "\n",
    "    print(f\"Loaded {len(output)} β posterior archives from {folder}\")\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# largest_mean_pi_norm.py\n",
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "\n",
    "def _as_ndarray(obj) -> np.ndarray:\n",
    "    \"\"\"Return obj as a NumPy array, handling xarray objects transparently.\"\"\"\n",
    "    if isinstance(obj, (np.ndarray, list, tuple)):\n",
    "        return np.asarray(obj)\n",
    "    if isinstance(obj, (xr.DataArray, xr.Dataset)):\n",
    "        return obj.values\n",
    "    raise TypeError(\n",
    "        \"Expected NumPy array-like or xarray object for `pis`, \"\n",
    "        f\"got {type(obj).__name__}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def largest_mean_pi_norm(\n",
    "    idata,\n",
    "    *,\n",
    "    pis: Any | None = None,\n",
    "    pis_var: str = \"pis\",\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    For every source s\n",
    "    ---------------------------------\n",
    "    1. find the component c that maximises the posterior mean of\n",
    "       `pi_norm[s, c]`;\n",
    "    2. also report the *true* largest proportion, i.e.\n",
    "       max_c pis[s, c].\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    idata : arviz.InferenceData\n",
    "        Must contain posterior variable 'pi_norm'.\n",
    "    pis : array-like or xarray.DataArray, optional\n",
    "        Ground-truth mixing proportions with shape (n_sources, n_components).\n",
    "        If not supplied the function tries to read `idata.posterior[pis_var]`.\n",
    "    pis_var : str\n",
    "        Name of the variable inside `idata.posterior` that stores `pis`\n",
    "        (ignored if `pis` argument is provided).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[dict]\n",
    "        One dictionary per source with keys\n",
    "        • source          – int (0-based)\n",
    "        • component       – int, index of argmax of mean π\n",
    "        • mean_value      – float, posterior mean π at that (s, c)\n",
    "        • true_proportion – float, max_c pis[s, c] if available, else None\n",
    "    \"\"\"\n",
    "    # ── posterior mean of pi_norm ―――――――――――――――――――――――――――――――――――――――\n",
    "    mean_pi = idata.posterior[\"pi_norm\"].mean(dim=(\"chain\", \"draw\"))  # (source, component)\n",
    "    mean_vals = mean_pi.values                                        # ndarray (S, C)\n",
    "\n",
    "    # best component index for each source\n",
    "    best_comp_idx = mean_vals.argmax(axis=1)                          # shape (S,)\n",
    "\n",
    "    # ── fetch / validate true proportions ――――――――――――――――――――――――――――――\n",
    "    if pis is None:\n",
    "        if pis_var in idata.posterior:\n",
    "            pis = idata.posterior[pis_var]\n",
    "        else:\n",
    "            pis = None\n",
    "\n",
    "    pis_vals = None\n",
    "    if pis is not None:\n",
    "        pis_vals = _as_ndarray(pis)\n",
    "        if pis_vals.shape != mean_vals.shape:\n",
    "            raise ValueError(\n",
    "                \"Shape mismatch between `pis` \"\n",
    "                f\"{pis_vals.shape} and pi_norm mean {mean_vals.shape}\"\n",
    "            )\n",
    "\n",
    "    # ── assemble results ――――――――――――――――――――――――――――――――――――――――――――――\n",
    "    results = []\n",
    "    for src, comp in enumerate(best_comp_idx):\n",
    "        true_prop = None\n",
    "        if pis_vals is not None:\n",
    "            true_prop = float(pis_vals[src].max())\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"source\": src,\n",
    "                \"component\": int(comp),\n",
    "                \"mean_value\": float(mean_vals[src, comp]),\n",
    "                \"true_proportion\": true_prop,\n",
    "            }\n",
    "        )\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "18f79a3fb006cfef",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "beta_store = store_beta_posteriors(\n",
    "    idata=idata,\n",
    "    df_sim=df_sim,\n",
    "    output_folder=\"results/beta_archives\",\n",
    "    file_ext=\"npz\",      # or \"npy\"\n",
    "    compress=True,\n",
    ")\n",
    "\n",
    "beta_data = load_beta_posteriors(\n",
    "    folder=\"results/beta_archives\",\n",
    "    file_ext=\"npz\",     # or \"npy\" if you saved .npy files\n",
    ")\n",
    "\n",
    "# Access arrays for source 3\n",
    "b3 = beta_data[\"beta_3\"]\n",
    "samples_b3 = b3[\"samples\"]\n",
    "grid_b3    = b3[\"grid\"]\n",
    "loglik_b3  = b3[\"loglik\"]\n",
    "\n",
    "best_per_source = largest_mean_pi_norm(idata, pis=pis)  # if you have one\n",
    "\n",
    "for rec in best_per_source:\n",
    "    print(\n",
    "        f\"Source {rec['source']:>2}: \"\n",
    "        f\"best component={rec['component']:>2}, \"\n",
    "        f\"mean π={rec['mean_value']:.3f}, \"\n",
    "        f\"true max π={rec['true_proportion']:.3f}\"\n",
    "    )\n"
   ],
   "id": "367c81dccdb17d2f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "save_beta_density_hist(\n",
    "    idata,\n",
    "    output_folder=\"figures/beta_plots\",\n",
    "    experiment=\"sim1\",          # will create sim1_beta_0.jpg, …\n",
    ")"
   ],
   "id": "33d93bafafa970a6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "plot_df_prior_vs_posterior(df_sim, idata)",
   "id": "67bd5ee502b6fc09"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "plot_df_prior_vs_posterior( idata, beta_df)",
   "id": "7192284a16f41748"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "kl_vals = kl_prior_posterior_beta(df_sim, idata)\n",
    "for s, kl in kl_vals.items():\n",
    "    print(f\"Source {s}: KL(posterior‖prior) = {kl:0.4f}\")\n"
   ],
   "id": "ef9e82e08eef89cc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "stats = summarise_beta_prior_posterior(df_sim, idata,1)\n",
    "print(stats)\n",
    "\n",
    "# save to csv\n",
    "stats.to_csv(\"beta_prior_posterior_stats.csv\", index=False)\n"
   ],
   "id": "b1e73eda62482a6b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "'''\n",
    "def summarise_beta_prior_posterior_sample(\n",
    "        df_sim,\n",
    "        idata,\n",
    "        experiment: int = 1\n",
    "):\n",
    "    \"\"\"\n",
    "    For each β_s parameter compute\n",
    "\n",
    "        • prior sample mean & median (raw `point` column of df_sim)\n",
    "        • posterior sample mean, median, 95 % CI\n",
    "        • flag : does the 95 % CI cover the prior sample mean?\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame with columns\n",
    "        source\n",
    "        prior_mean prior_median\n",
    "        post_mean post_median\n",
    "        post_ci_lower post_ci_upper\n",
    "        ci_covers_prior_mean\n",
    "    \"\"\"\n",
    "    # ── locate β_s in the posterior ───────────────────────────────\n",
    "    beta_vars = [v for v in idata.posterior.data_vars\n",
    "                 if re.fullmatch(r\"beta_\\d+\", v)]\n",
    "    sources_avl = sorted(int(v.split(\"_\")[1]) for v in beta_vars)\n",
    "    if not sources_avl:\n",
    "        raise ValueError(\"No beta_* variables found in idata.posterior\")\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for s in sources_avl:\n",
    "        # ── PRIOR sample statistics (raw grid points) -------------\n",
    "        prior_samples = df_sim.loc[df_sim[\"source\"] == s, \"point\"].values\n",
    "        prior_mean = float(np.mean(prior_samples))\n",
    "        prior_median = float(np.median(prior_samples))\n",
    "\n",
    "        # ── POSTERIOR sample statistics ---------------------------\n",
    "        post_samples = idata.posterior[f\"beta_{s}\"].values.flatten()\n",
    "        post_mean = float(np.mean(post_samples))\n",
    "        post_median = float(np.median(post_samples))\n",
    "        ci_lower, ci_upper = np.percentile(post_samples, [2.5, 97.5])\n",
    "\n",
    "        covers = (ci_lower <= prior_mean) and (prior_mean <= ci_upper)\n",
    "\n",
    "        rows.append(dict(\n",
    "            experiment=experiment,\n",
    "            source=s,\n",
    "            prior_mean=prior_mean,\n",
    "            prior_median=prior_median,\n",
    "            post_mean=post_mean,\n",
    "            post_median=post_median,\n",
    "            post_ci_lower=ci_lower,\n",
    "            post_ci_upper=ci_upper,\n",
    "            ci_covers_prior_mean=covers,\n",
    "        ))\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "stats = summarise_beta_prior_posterior(df_sim, idata, 1)\n",
    "print(stats)\n",
    "'''"
   ],
   "id": "b98dc6b7b489326b"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-06-08T22:41:15.924623Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jax.random as random\n",
    "import jax.numpy as jnp\n",
    "import arviz as az\n",
    "from numpyro.infer import MCMC, NUTS\n",
    "from datetime import date\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "#  Main driver\n",
    "# ------------------------------------------------------------------\n",
    "def run_hdp_experiments(\n",
    "    K: int,\n",
    "    O: int,\n",
    "    S: int,\n",
    "    n_obs: int,\n",
    "    N_sources: int,\n",
    "    seed: int,\n",
    "    output_base: str,\n",
    "    *,\n",
    "    num_warmup: int = 5000,\n",
    "    num_samples: int = 20000,\n",
    "    n_reps: int = 10,                 # ← how many repetitions (k times)\n",
    "):\n",
    "    \"\"\"\n",
    "    Run the complete HDP simulation / fitting experiment `n_reps` times for\n",
    "    *each* of the three data-generating families: linear, Poisson, logistic.\n",
    "\n",
    "    All artefacts are written beneath\n",
    "\n",
    "        {output_base}/{model_type}/\n",
    "\n",
    "    where `model_type` ∈ {\"linear\",\"poisson\",\"logistic\"}.\n",
    "    A single CSV containing the concatenated summary statistics is produced\n",
    "    for every model family.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    K, O, S, n_obs\n",
    "        Mixture, outcome & observation sizes to pass through to your\n",
    "        simulate_profile_likelihoods_* helpers.\n",
    "    N_sources\n",
    "        Preset number of sources in the HDP.\n",
    "    seed\n",
    "        Global seed.  Repetition-specific seeds are derived from it so that no\n",
    "        two repetitions ever share the same random stream.\n",
    "    output_base\n",
    "        Root directory in which all results will be stored.\n",
    "    num_warmup, num_samples\n",
    "        Sampler settings for the NUTS-based MCMC.\n",
    "    n_reps\n",
    "        Number of repetitions **per model family**.\n",
    "    \"\"\"\n",
    "\n",
    "    today_str = date.today().isoformat()\n",
    "    rng_master = np.random.default_rng(seed)\n",
    "\n",
    "    # Map each model family to its simulator helper ---------------------------\n",
    "    sim_fns = dict(\n",
    "        linear   = simulate_profile_likelihoods_linreg,\n",
    "        poisson  = simulate_profile_likelihoods_poisson,\n",
    "        logistic = simulate_profile_likelihoods_logistic,\n",
    "    )\n",
    "\n",
    "    for model_type, sim_fn in sim_fns.items():\n",
    "        model_dir = Path(output_base) / model_type\n",
    "        fig_dir   = model_dir / \"figures\" / \"beta_plots\"\n",
    "        data_dir  = model_dir / \"data\"\n",
    "        fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        stats_all   = []   # collects one DF per repetition\n",
    "        kl_all      = []\n",
    "        proportion_largest = []\n",
    "\n",
    "        for rep in range(1, n_reps + 1):\n",
    "            # ----------------------------------------------------------------\n",
    "            # 1)  Data generation\n",
    "            # ----------------------------------------------------------------\n",
    "            rep_seed = rng_master.integers(seed+rep)\n",
    "            rng_rep  = np.random.default_rng(rep_seed)\n",
    "\n",
    "            grid_pts  = np.linspace(-10, 10, 100)\n",
    "            beta_mean = rng_rep.normal(loc=1.5, scale=0.7, size=K)\n",
    "            beta_sds  = np.abs(rng_rep.normal(loc=0.0, scale=1.0, size=K))\n",
    "\n",
    "            # Simulator may have family-specific keyword parameters.\n",
    "            sim_kwargs = dict(\n",
    "                K=K, S=S, O=O, n_obs=n_obs,\n",
    "                beta_mean=beta_mean,\n",
    "                beta_sds=beta_sds,\n",
    "                grid=grid_pts,\n",
    "                seed=rep_seed,\n",
    "            )\n",
    "            if model_type == \"linear\":\n",
    "                sim_kwargs[\"sigma_noise\"] = 0.8\n",
    "\n",
    "            df_sim, pis_true, df_betas = sim_fn(**sim_kwargs)\n",
    "\n",
    "            # ----------------------------------------------------------------\n",
    "            # 2)  Reshape into source-outcome dict expected by HDP_model\n",
    "            # ----------------------------------------------------------------\n",
    "            source_outcome_data = {}\n",
    "            for s in range(1, N_sources + 1):\n",
    "                df_s = df_sim[df_sim[\"source\"] == s]\n",
    "                outcome_list = []\n",
    "                for o in sorted(df_s[\"outcome\"].unique()):\n",
    "                    arr = df_s[df_s[\"outcome\"] == o][[\"point\", \"value\"]].to_numpy()\n",
    "                    outcome_list.append(jnp.array(arr))\n",
    "                source_outcome_data[s] = outcome_list\n",
    "\n",
    "            num_outcomes_dict = {s: len(source_outcome_data[s])\n",
    "                                 for s in range(1, N_sources + 1)}\n",
    "\n",
    "            # ----------------------------------------------------------------\n",
    "            # 3)  Run HDP\n",
    "            # ----------------------------------------------------------------\n",
    "            rng_key = random.PRNGKey(rep_seed)\n",
    "            data_point_mean = jnp.asarray(df_sim[\"point\"].mean())\n",
    "\n",
    "            nuts_kernel = NUTS(HDP_model)\n",
    "            mcmc = MCMC(nuts_kernel, num_warmup=num_warmup, num_samples=num_samples)\n",
    "            mcmc.run(\n",
    "                rng_key,\n",
    "                source_outcome_data=source_outcome_data,\n",
    "                num_outcomes_dict=num_outcomes_dict,\n",
    "                N_sources=N_sources,\n",
    "                k=K,\n",
    "                data_point_mean=data_point_mean,\n",
    "            )\n",
    "            idata = az.from_numpyro(mcmc)\n",
    "\n",
    "            store_beta_posteriors(\n",
    "                idata=idata,\n",
    "                df_sim=df_sim,\n",
    "                output_folder=data_dir/f\"rep{rep}\",\n",
    "                file_ext=\"npz\",      # or \"npy\"\n",
    "                compress=True,\n",
    "            )\n",
    "\n",
    "            # ----------------------------------------------------------------\n",
    "            # 4)  Save betas density histograms\n",
    "            # ----------------------------------------------------------------\n",
    "            save_beta_density_hist(\n",
    "                idata,\n",
    "                output_folder=str(fig_dir),\n",
    "                experiment=f\"rep{rep}\",\n",
    "            )\n",
    "\n",
    "            # ----------------------------------------------------------------\n",
    "            # 5)  Save PPC (prior vs posterior) comparison\n",
    "            # ----------------------------------------------------------------\n",
    "            ppc_path = model_dir / f\"rep{rep}_ppc.png\"\n",
    "            ppc_obj  = plot_df_prior_vs_posterior(df_sim, idata, save_to=ppc_path)\n",
    "\n",
    "\n",
    "            if isinstance(ppc_obj, plt.Figure):           # helper returns a Figure\n",
    "                ppc_obj.savefig(ppc_path, dpi=300, bbox_inches=\"tight\")\n",
    "                plt.close(ppc_obj)\n",
    "\n",
    "            elif hasattr(ppc_obj, \"figure\"):              # helper returns an Axes\n",
    "                ppc_obj.figure.savefig(ppc_path, dpi=300, bbox_inches=\"tight\")\n",
    "                plt.close(ppc_obj.figure)\n",
    "\n",
    "            else:\n",
    "                # Helper already saved the plot (or user doesn’t need a file).\n",
    "                # If you still want confirmation:\n",
    "                print(f\"  [info] plot_df_prior_vs_posterior returned {type(ppc_obj)}; \"\n",
    "                      \"skipping explicit save.\")\n",
    "\n",
    "            # ----------------------------------------------------------------\n",
    "            # 6)  KL divergence\n",
    "            # ----------------------------------------------------------------\n",
    "            kl_vals = kl_prior_posterior_beta(df_sim, idata)\n",
    "            kl_vals[\"rep\"] = rep          # keep track of repetition\n",
    "            kl_all.append(kl_vals)\n",
    "\n",
    "            # ----------------------------------------------------------------\n",
    "            # 7)  Summary statistics\n",
    "            # ----------------------------------------------------------------\n",
    "            stats = summarise_beta_prior_posterior(df_sim, idata, kl_vals, experiment_label=f\"rep{rep}\")\n",
    "            largest_info = largest_mean_pi_norm(idata, pis=pis)\n",
    "            largest_df   = pd.DataFrame(largest_info).rename(columns={\n",
    "                \"component\":      \"largest_component\",\n",
    "                \"mean_value\":     \"mean_pi_norm\",\n",
    "                \"true_proportion\":\"true_max_pi\",\n",
    "                \"source\":         \"src_zero\",\n",
    "            })\n",
    "\n",
    "            # 3 ── create the join key in *stats*  (convert “beta1” → 0, “beta2” → 1, …)\n",
    "            stats[\"src_zero\"] = stats[\"source\"].str.extract(r\"(\\d+)\").astype(int) - 1\n",
    "\n",
    "            # 4 ── merge and drop helper key\n",
    "            stats = (\n",
    "                stats\n",
    "                  .merge(largest_df, on=\"src_zero\", how=\"left\")\n",
    "                  .drop(columns=\"src_zero\")\n",
    "            )\n",
    "\n",
    "\n",
    "            stats_all.append(stats)\n",
    "\n",
    "            print(f\"[{model_type}] repetition {rep} finished.\")\n",
    "\n",
    "        # --------------------------------------------------------------------\n",
    "        #  Persist concatenated results for this model family\n",
    "        # --------------------------------------------------------------------\n",
    "        stats_df = pd.concat(stats_all, ignore_index=True)\n",
    "        fname = model_dir / f\"beta_summary_stats_{today_str}.csv\"\n",
    "        stats_df.to_csv(fname, index=False)\n",
    "\n",
    "\n",
    "        print(f\"\\nCompleted all {n_reps} repetitions for '{model_type}'. \"\n",
    "              f\"Results stored in: {model_dir}\\n\")\n",
    "\n",
    "run_hdp_experiments(\n",
    "    K=5, O=8, S=8, n_obs=120,\n",
    "    N_sources=8,\n",
    "    seed=42,\n",
    "    output_base=\"results\",\n",
    "    num_warmup=5000,\n",
    "    num_samples=20000,\n",
    "    n_reps=100\n",
    ")\n"
   ],
   "id": "86861127fdf30b22",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample:  26%|██▌       | 6432/25000 [00:40<03:12, 96.62it/s, 255 steps of size 1.17e-02. acc. prob=0.91]  "
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e9d15e3e8ec31cf3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
