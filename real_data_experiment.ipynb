{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-19T04:59:04.093130Z",
     "start_time": "2025-02-19T04:58:58.659116Z"
    }
   },
   "source": [
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pymc as pm\n",
    "import pytensor\n",
    "import pytensor.tensor as pt\n",
    "\n",
    "from pytensor.graph import Apply, Op\n",
    "from scipy.optimize import approx_fprime"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (pytensor.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "RANDOM_SEED = 42\n",
    "\n",
    "def stick_breaking(beta):\n",
    "    portion_remaining = pt.concatenate([[1], pt.extra_ops.cumprod(1 - beta)[:-1]])\n",
    "    return beta * portion_remaining\n",
    "\n",
    "def reparameterize(pi):\n",
    "    return pi / pi.sum()\n",
    "\n",
    "def hierarchical_beta(alpha0, beta, nsources, k):\n",
    "    \"\"\"Hierarchical Beta distribution for multiple sources.\"\"\"\n",
    "    pi_tilt_sources = []\n",
    "    \n",
    "    for s in range(nsources):\n",
    "        beta_params = [(alpha0 * beta[k], alpha0 * (1 - pm.math.sum(beta[:k + 1]))) for k in range(k)]\n",
    "        pi_tilt = pm.Beta(f'pi_tilt_{s}', \n",
    "                          alpha=[b[0] for b in beta_params], \n",
    "                          beta=[b[1] for b in beta_params], \n",
    "                          dims=\"component\")\n",
    "        pi_tilt_sources.append(pi_tilt)\n",
    "    \n",
    "    #return pm.math.stack(pi_tilt_sources, axis=0)\n",
    "    return pi_tilt_sources"
   ],
   "id": "1c95b40a68db948a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pytensor.tensor as pt\n",
    "from pytensor.graph.op import Op, Apply\n",
    "import pymc as pm\n",
    "import pandas as pd\n",
    "\n",
    "# Define interpolate function\n",
    "def interpolate(x0, y0, x):\n",
    "    x = np.array(x)\n",
    "    idx = np.searchsorted(x0, x)\n",
    "    dl = np.array(x - x0[idx - 1])\n",
    "    dr = np.array(x0[idx] - x)\n",
    "    d = dl + dr\n",
    "    wl = dr / d\n",
    "    return wl * y0[idx - 1] + (1 - wl) * y0[idx]\n",
    "\n",
    "# Load data and ensure it's a NumPy array\n",
    "data = pd.read_csv('grids_example_1.csv', header=None).values\n",
    "param_min = data[0, :].min()\n",
    "param_max = data[0, :].max()\n",
    "\n",
    "# Function to compute log-likelihood    \n",
    "def my_loglike(x, data):\n",
    "    x_vals = data[0, :]\n",
    "    loglike_vals = data[1, :]\n",
    "    return interpolate(x_vals, loglike_vals, x)\n",
    "\n",
    "# Define custom PyTensor operation\n",
    "class LogLike(Op):\n",
    "    def make_node(self, x, data):\n",
    "        x = pt.as_tensor_variable(x)  # Ensure x is a tensor\n",
    "        data = pt.as_tensor_variable(data)  # Ensure data is a tensor\n",
    "        inputs = [x, data]\n",
    "        outputs = [pt.TensorType(dtype=\"float64\", broadcastable=(False,))()]\n",
    "        return Apply(self, inputs, outputs)\n",
    "\n",
    "    def perform(self, node, inputs, outputs):\n",
    "        x, data = inputs\n",
    "        x_vals = data[0, :]\n",
    "        loglike_vals = data[1, :]\n",
    "        loglike_eval = interpolate(x_vals, loglike_vals, x)\n",
    "        outputs[0][0] = np.array(loglike_eval)\n",
    "\n",
    "# Initialize operation\n",
    "loglike_op = LogLike()\n",
    "\n",
    "# PyMC model without observed data\n",
    "with pm.Model() as no_grad_model:\n",
    "    # Define prior for x\n",
    "    x = pm.Uniform(\"x\", lower=param_min, upper=param_max, shape=1)\n",
    "\n",
    "    # Add a custom potential for the likelihood\n",
    "    pm.Potential(\"likelihood\", loglike_op(x, data))\n",
    "\n",
    "    # Sample posterior\n",
    "    idata = pm.sample(\n",
    "        200000, \n",
    "        tune=5000,\n",
    "        chains=8,\n",
    "    )"
   ],
   "id": "d2eec1ff058c9ae0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pytensor.tensor as pt\n",
    "from pytensor.graph.op import Op, Apply\n",
    "import pymc as pm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define interpolate function\n",
    "def interpolate(x0, y0, x):\n",
    "    x = np.array(x)\n",
    "    idx = np.searchsorted(x0, x)\n",
    "    dl = np.array(x - x0[idx - 1])\n",
    "    dr = np.array(x0[idx] - x)\n",
    "    d = dl + dr\n",
    "    wl = dr / d\n",
    "    return wl * y0[idx - 1] + (1 - wl) * y0[idx]\n",
    "\n",
    "# Load data and ensure it's a NumPy array\n",
    "data = pd.read_csv('grids_example_1.csv', header=None).values\n",
    "param_min = data[0, :].min()\n",
    "param_max = data[0, :].max()"
   ],
   "id": "63627e3544dc7fcf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# plot the data\n",
    "plt.plot(data[0, :], data[1, :], 'o')\n",
    "\n",
    "plt.show()"
   ],
   "id": "f772f4f7da646e72",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **1. Interpolation of Log-Likelihoods**\n",
    "For each outcome \\( j \\), we interpolate the log-likelihood for each component \\( k \\):\n",
    "\n",
    "$$\\hat{\\ell}_{j,k} = \\text{interpolate}(x_{\\text{grid},j}, \\ell_{\\text{grid},j}, \\beta_{j,k})$$\n",
    "\n",
    "\n",
    "$$\\ell_k = \\sum_{j=1}^{N_{\\text{outcomes}}} \\hat{\\ell}_{j,k}$$\n",
    "\n",
    "$$\\ell_{\\text{final}} = \\log \\sum_{k=1}^{N_{\\text{components}}} \\pi_k e^{\\ell_k}$$\n"
   ],
   "id": "c38e89b51668f407"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T05:23:23.447963Z",
     "start_time": "2025-02-19T05:23:23.422383Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytensor.tensor as pt\n",
    "from pytensor.graph.op import Op, Apply\n",
    "import pymc as pm\n",
    "\n",
    "# Load profile likelihood data\n",
    "data = pd.read_csv(\"profileLikelihoods_NCs_long.csv\")\n",
    "\n",
    "# Define interpolation function\n",
    "def interpolate(x0, y0, x):\n",
    "    x = np.array(x)\n",
    "    idx = np.searchsorted(x0, x)\n",
    "    dl = np.array(x - x0[idx - 1])\n",
    "    dr = np.array(x0[idx] - x)\n",
    "    d = dl + dr\n",
    "    wl = dr / d\n",
    "    return wl * y0[idx - 1] + (1 - wl) * y0[idx]\n",
    "\n",
    "# Define custom likelihood function\n",
    "class LogLike(Op):\n",
    "    def make_node(self, β, num_outcomes, source_data):\n",
    "        β = pt.as_tensor(β)  # Ensure scalar\n",
    "        num_outcomes = pt.as_tensor(int(num_outcomes))  # Ensure scalar integer\n",
    "        source_data = pt.as_tensor(np.asarray(source_data))  # Ensure NumPy array\n",
    "\n",
    "        # The output must be a single scalar\n",
    "        outputs = [pt.dscalar()]\n",
    "        return Apply(self, [β, num_outcomes, source_data], outputs)\n",
    "\n",
    "    def perform(self, node, inputs, outputs):\n",
    "        β, num_outcomes, source_data = inputs\n",
    "        num_outcomes = int(num_outcomes)  # Ensure integer\n",
    "\n",
    "        # Compute total log-likelihood (scalar β)\n",
    "        total_log_likelihood = 0.0\n",
    "\n",
    "        for j in range(num_outcomes):\n",
    "            # Extract outcome-specific x_vals and loglike_vals\n",
    "            outcome_data = source_data[source_data[:, 2] == (j+1)]  # Filter by outcome index\n",
    "            \n",
    "            if outcome_data.shape[0] == 0:\n",
    "                raise ValueError(f\"No data found for outcome {j+1} in source_data!\")\n",
    "\n",
    "            x_vals = outcome_data[:, 0]  # Parameter grid points\n",
    "            loglike_vals = outcome_data[:, 1]  # Log-likelihood values\n",
    "\n",
    "            # Interpolate the log-likelihood for β\n",
    "            loglike_j = interpolate(x_vals, loglike_vals, β)\n",
    "\n",
    "            # Sum in log-space\n",
    "            total_log_likelihood += loglike_j\n",
    "\n",
    "        # Ensure the output is a scalar\n",
    "        outputs[0][0] = np.array(total_log_likelihood)\n",
    "\n",
    "# Initialize the custom operation\n",
    "loglike_op = LogLike()\n",
    "\n",
    "# Simulated test case for source 1\n",
    "n_outcomes_test = int(data[\"outcome\"].nunique())  # Ensure integer\n",
    "n_components = 3  # Assume 3 components for illustration\n",
    "\n",
    "np.random.seed(42)\n",
    "β_test = np.random.uniform(low=-1, high=-0.5)  # Simulated β values\n",
    "num_outcomes_test = int(data[data[\"source\"] == 1][\"outcome\"].nunique())  # Ensure integer\n",
    "source_1_data = data[data[\"source\"] == 1][[\"point\", \"value\", \"outcome\"]].values  # Include outcome column\n",
    "weights_test = np.full(n_components, 1 / n_components)  # Equal mixture weights\n",
    "\n",
    "\n",
    "test_out = loglike_op(β_test, num_outcomes_test, source_1_data)\n",
    "print(\"Evaluated Test Likelihood Output:\", test_out.eval())\n"
   ],
   "id": "385ca4107aaf8b2c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated Test Likelihood Output: -30.351530816420613\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T04:59:21.748832Z",
     "start_time": "2025-02-19T04:59:21.743011Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.random.seed(42)\n",
    "β_test = np.random.uniform(low=-1, high=-0.5)  # Simulated β values\n",
    "num_outcomes_test = int(data[data[\"source\"] == 1][\"outcome\"].nunique())  # Ensure integer\n",
    "source_1_data = data[data[\"source\"] == 1][[\"point\", \"value\", \"outcome\"]].values  # Include outcome column\n",
    "weights_test = np.full(n_components, 1 / n_components)  # Equal mixture weights"
   ],
   "id": "bf6cc4850c9fda36",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T05:23:26.917175Z",
     "start_time": "2025-02-19T05:23:26.911576Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def stick_breaking(beta):\n",
    "    portion_remaining = pt.concatenate([[1], pt.extra_ops.cumprod(1 - beta)[:-1]])\n",
    "    return beta * portion_remaining\n",
    "\n",
    "def reparameterize(pi):\n",
    "    return pi / pi.sum()\n",
    "\n",
    "def hierarchical_beta(alpha0, beta, nsources, k):\n",
    "    \"\"\"Hierarchical Beta distribution for multiple sources.\"\"\"\n",
    "    pi_tilt_sources = []\n",
    "    \n",
    "    for s in range(nsources):\n",
    "        beta_params = [(alpha0 * beta[k], alpha0 * (1 - pm.math.sum(beta[:k + 1]))) for k in range(k)]\n",
    "        pi_tilt = pm.Beta(f'pi_tilt_{s}', \n",
    "                          alpha=[b[0] for b in beta_params], \n",
    "                          beta=[b[1] for b in beta_params], \n",
    "                          dims=\"component\")\n",
    "        pi_tilt_sources.append(pi_tilt)\n",
    "    \n",
    "    #return pm.math.stack(pi_tilt_sources, axis=0)\n",
    "    return pi_tilt_sources"
   ],
   "id": "d12ff7b9b82240cf",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-02-19T05:51:28.335523Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pymc as pm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load profile likelihood data\n",
    "data = pd.read_csv(\"profileLikelihoods_NCs_long.csv\")\n",
    "\n",
    "# Constants\n",
    "N_sources = data[\"source\"].nunique()  # Number of sources\n",
    "k = 5  # Number of mixture components\n",
    "\n",
    "# Precompute source data and number of outcomes\n",
    "source_data_dict = {}\n",
    "num_outcomes_dict = {}\n",
    "\n",
    "for s in range(1, N_sources + 1):  # Ensure correct indexing\n",
    "    source_data_dict[s] = data[data[\"source\"] == s][[\"point\", \"value\", \"outcome\"]].astype(float).values\n",
    "    num_outcomes_dict[s] = int(data[data[\"source\"] == s][\"outcome\"].nunique())\n",
    "\n",
    "# PyMC Model\n",
    "with pm.Model(coords={\"component\": np.arange(k), \"n_source\": np.arange(N_sources)}) as HDP_model:\n",
    "\n",
    "    # Priors for hierarchical Dirichlet process weights\n",
    "    gamma = pm.Gamma(\"gamma\", 1.0, 5.0)\n",
    "    alpha0 = pm.Gamma(\"alpha0\", 1.0, 5.0)\n",
    "    \n",
    "    # Stick-breaking process for mixture weights\n",
    "    beta_tilt = pm.Beta(\"beta_tilt\", 1.0, gamma, dims=\"component\")\n",
    "    beta = pm.Deterministic(\"beta\", stick_breaking(beta_tilt), dims=\"component\")\n",
    "    π_tilt = hierarchical_beta(alpha0, beta, nsources=N_sources, k=k)\n",
    "\n",
    "    π_norms = []\n",
    "    for s in range(N_sources):\n",
    "        π = pm.Deterministic(f\"π_{s}\", stick_breaking(π_tilt[s]), dims=\"component\")\n",
    "        π_norms.append(pm.Deterministic(f\"π_norm_{s}\", reparameterize(π), dims=[\"component\"]))\n",
    "\n",
    "    # Mixture component parameters\n",
    "    μ = pm.Normal(\n",
    "        \"μ\",\n",
    "        mu=data[\"point\"].mean(),  # Use the mean of the profile likelihood grid points\n",
    "        sigma=10,\n",
    "        shape=k,\n",
    "        transform=pm.distributions.transforms.ordered,  # Ensure μ is ordered\n",
    "        initval=np.linspace(-1, 1, k),  # Reasonable ordered starting values\n",
    "    )\n",
    "\n",
    "    σ = pm.HalfNormal(\"σ\", sigma=10, shape=k)\n",
    "\n",
    "    # Likelihood for each source\n",
    "    for s in range(1, N_sources + 1):  # Ensure correct indexing\n",
    "        # Draw β from the mixture model\n",
    "        β = pm.NormalMixture(f'β_{s}', w=π_norms[s-1], mu=μ, sigma=σ)\n",
    "        βt = pm.Deterministic(f\"β_clipped_{s}\", pm.math.clip(β, source_data[:, 0].min(), source_data[:, 0].max()))\n",
    "        #βt = pm.Truncated(f\"β_truncated_{s}\", β , lower=source_data[:, 0].min(), upper=source_data[:, 0].max())\n",
    "        # Use precomputed values\n",
    "        num_outcomes = num_outcomes_dict[s]\n",
    "        source_data = source_data_dict[s]\n",
    "\n",
    "        likelihood = pm.Potential(f\"loglike_{s}\", loglike_op(βt, num_outcomes, source_data))\n",
    "\n",
    "    # Sampling\n",
    "    trace = pm.sample(\n",
    "        tune=5000,\n",
    "        draws=20000,\n",
    "        init=\"advi\",\n",
    "        #step=pm.Metropolis(),  # Use Metropolis for non-differentiable likelihood\n",
    "        random_seed=42,\n",
    "    )\n",
    "\n"
   ],
   "id": "7fec6f99099033a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ca3fc04b92932993"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
