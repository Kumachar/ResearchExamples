{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pymc as pm\n",
    "import pytensor\n",
    "import pytensor.tensor as pt\n",
    "\n",
    "from pytensor.graph import Apply, Op\n",
    "from scipy.optimize import approx_fprime"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "RANDOM_SEED = 42\n",
    "\n",
    "def stick_breaking(beta):\n",
    "    portion_remaining = pt.concatenate([[1], pt.extra_ops.cumprod(1 - beta)[:-1]])\n",
    "    return beta * portion_remaining\n",
    "\n",
    "def reparameterize(pi):\n",
    "    return pi / pi.sum()\n",
    "\n",
    "def hierarchical_beta(alpha0, beta, nsources, k):\n",
    "    \"\"\"Hierarchical Beta distribution for multiple sources.\"\"\"\n",
    "    pi_tilt_sources = []\n",
    "    \n",
    "    for s in range(nsources):\n",
    "        beta_params = [(alpha0 * beta[k], alpha0 * (1 - pm.math.sum(beta[:k + 1]))) for k in range(k)]\n",
    "        pi_tilt = pm.Beta(f'pi_tilt_{s}', \n",
    "                          alpha=[b[0] for b in beta_params], \n",
    "                          beta=[b[1] for b in beta_params], \n",
    "                          dims=\"component\")\n",
    "        pi_tilt_sources.append(pi_tilt)\n",
    "    \n",
    "    #return pm.math.stack(pi_tilt_sources, axis=0)\n",
    "    return pi_tilt_sources"
   ],
   "id": "1c95b40a68db948a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pytensor.tensor as pt\n",
    "from pytensor.graph.op import Op, Apply\n",
    "import pymc as pm\n",
    "import pandas as pd\n",
    "\n",
    "# Define interpolate function\n",
    "def interpolate(x0, y0, x):\n",
    "    x = np.array(x)\n",
    "    idx = np.searchsorted(x0, x)\n",
    "    dl = np.array(x - x0[idx - 1])\n",
    "    dr = np.array(x0[idx] - x)\n",
    "    d = dl + dr\n",
    "    wl = dr / d\n",
    "    return wl * y0[idx - 1] + (1 - wl) * y0[idx]\n",
    "\n",
    "# Load data and ensure it's a NumPy array\n",
    "data = pd.read_csv('grids_example_1.csv', header=None).values\n",
    "param_min = data[0, :].min()\n",
    "param_max = data[0, :].max()\n",
    "\n",
    "# Function to compute log-likelihood    \n",
    "def my_loglike(x, data):\n",
    "    x_vals = data[0, :]\n",
    "    loglike_vals = data[1, :]\n",
    "    return interpolate(x_vals, loglike_vals, x)\n",
    "\n",
    "# Define custom PyTensor operation\n",
    "class LogLike(Op):\n",
    "    def make_node(self, x, data):\n",
    "        x = pt.as_tensor_variable(x)  # Ensure x is a tensor\n",
    "        data = pt.as_tensor_variable(data)  # Ensure data is a tensor\n",
    "        inputs = [x, data]\n",
    "        outputs = [pt.TensorType(dtype=\"float64\", broadcastable=(False,))()]\n",
    "        return Apply(self, inputs, outputs)\n",
    "\n",
    "    def perform(self, node, inputs, outputs):\n",
    "        x, data = inputs\n",
    "        x_vals = data[0, :]\n",
    "        loglike_vals = data[1, :]\n",
    "        loglike_eval = interpolate(x_vals, loglike_vals, x)\n",
    "        outputs[0][0] = np.array(loglike_eval)\n",
    "\n",
    "# Initialize operation\n",
    "loglike_op = LogLike()\n",
    "\n",
    "# PyMC model without observed data\n",
    "with pm.Model() as no_grad_model:\n",
    "    # Define prior for x\n",
    "    x = pm.Uniform(\"x\", lower=param_min, upper=param_max, shape=1)\n",
    "\n",
    "    # Add a custom potential for the likelihood\n",
    "    pm.Potential(\"likelihood\", loglike_op(x, data))\n",
    "\n",
    "    # Sample posterior\n",
    "    idata = pm.sample(\n",
    "        200000, \n",
    "        tune=5000,\n",
    "        chains=8,\n",
    "    )"
   ],
   "id": "d2eec1ff058c9ae0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pytensor.tensor as pt\n",
    "from pytensor.graph.op import Op, Apply\n",
    "import pymc as pm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define interpolate function\n",
    "def interpolate(x0, y0, x):\n",
    "    x = np.array(x)\n",
    "    idx = np.searchsorted(x0, x)\n",
    "    dl = np.array(x - x0[idx - 1])\n",
    "    dr = np.array(x0[idx] - x)\n",
    "    d = dl + dr\n",
    "    wl = dr / d\n",
    "    return wl * y0[idx - 1] + (1 - wl) * y0[idx]\n",
    "\n",
    "# Load data and ensure it's a NumPy array\n",
    "data = pd.read_csv('grids_example_1.csv', header=None).values\n",
    "param_min = data[0, :].min()\n",
    "param_max = data[0, :].max()"
   ],
   "id": "63627e3544dc7fcf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# plot the data\n",
    "plt.plot(data[0, :], data[1, :], 'o')\n",
    "\n",
    "plt.show()"
   ],
   "id": "f772f4f7da646e72",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T05:07:02.730005Z",
     "start_time": "2025-02-18T05:07:02.699295Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytensor.tensor as pt\n",
    "from pytensor.graph.op import Op, Apply\n",
    "import pymc as pm\n",
    "\n",
    "# Load profile likelihood data\n",
    "data = pd.read_csv(\"profileLikelihoods_NCs_long.csv\")\n",
    "\n",
    "# Define interpolation function\n",
    "def interpolate(x0, y0, x):\n",
    "    x = np.array(x)\n",
    "    \n",
    "    if x0.size == 0 or y0.size == 0:\n",
    "        raise ValueError(\"Empty data passed to interpolation!\")\n",
    "\n",
    "    idx = np.searchsorted(x0, x) - 1  # Ensure index is within bounds\n",
    "    idx = np.clip(idx, 0, len(x0) - 2)  # Clip to avoid out-of-range errors\n",
    "    \n",
    "    dl = x - x0[idx]\n",
    "    dr = x0[idx + 1] - x\n",
    "    d = dl + dr\n",
    "\n",
    "    # Prevent division by zero\n",
    "    wl = np.where(d != 0, dr / d, 0.5)  \n",
    "    return wl * y0[idx] + (1 - wl) * y0[idx + 1]\n",
    "\n",
    "# Define custom likelihood function\n",
    "class LogLike(Op):\n",
    "    def make_node(self, βs, num_outcomes, source_data, weights):\n",
    "        βs = pt.as_tensor_variable(np.asarray(βs))  # Ensure array\n",
    "        num_outcomes = pt.as_tensor_variable(int(num_outcomes))  # Ensure scalar integer\n",
    "        source_data = pt.as_tensor_variable(np.asarray(source_data))  # Ensure array\n",
    "        weights = pt.as_tensor_variable(np.asarray(weights))  # Ensure array\n",
    "\n",
    "        # The output must be a single scalar\n",
    "        outputs = [pt.dscalar()]\n",
    "        return Apply(self, [βs, num_outcomes, source_data, weights], outputs)\n",
    "\n",
    "    def perform(self, node, inputs, outputs):\n",
    "        βs, num_outcomes, source_data, weights = inputs\n",
    "        num_outcomes = int(num_outcomes)  # Ensure integer\n",
    "\n",
    "        # Compute likelihoods for each outcome\n",
    "        total_likelihood = np.zeros(num_outcomes)\n",
    "\n",
    "        for j in range(num_outcomes):\n",
    "            β_j = βs[j]  # Get β for outcome j\n",
    "\n",
    "            # Extract outcome-specific x_vals and loglike_vals\n",
    "            outcome_data = source_data[source_data[:, 2] == (j+1)]  # Filter by outcome index\n",
    "            \n",
    "            if outcome_data.shape[0] == 0:\n",
    "                raise ValueError(f\"No data found for outcome {j+1} in source_data!\")\n",
    "\n",
    "            x_vals = outcome_data[:, 0]  # Parameter grid points\n",
    "            loglike_vals = outcome_data[:, 1]  # Log-likelihood values\n",
    "\n",
    "            # Interpolate for each component (β_j has n_components)\n",
    "            likelihoods_j = np.array([interpolate(x_vals, loglike_vals, β) for β in β_j])\n",
    "\n",
    "            # Weighted sum over components\n",
    "            total_likelihood[j] = np.dot(weights, likelihoods_j)\n",
    "\n",
    "        # Sum over all outcomes to get the final likelihood\n",
    "        final_likelihood = np.sum(total_likelihood)\n",
    "\n",
    "        # Ensure the output is a scalar\n",
    "        outputs[0][0] = np.array(final_likelihood)\n",
    "\n",
    "# Initialize the custom operation\n",
    "loglike_op = LogLike()\n",
    "\n",
    "# Simulated test case for source 1\n",
    "n_outcomes_test = int(data[\"outcome\"].nunique())  # Ensure integer\n",
    "n_components = 3  # Assume 3 components for illustration\n",
    "\n",
    "βs_test = np.random.uniform(low=-1, high=-0.5, size=(n_outcomes_test, n_components))  # Simulated β values\n",
    "num_outcomes_test = int(data[data[\"source\"] == 1][\"outcome\"].nunique())  # Ensure integer\n",
    "source_1_data = data[data[\"source\"] == 1][[\"point\", \"value\", \"outcome\"]].values  # Include outcome column\n",
    "weights_test = np.full(n_components, 1 / n_components)  # Equal mixture weights\n",
    "\n",
    "\n",
    "test_out = loglike_op(βs_test, num_outcomes_test, source_1_data, weights_test)\n",
    "print(\"Evaluated Test Likelihood Output:\", test_out.eval())\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "385ca4107aaf8b2c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated Test Likelihood Output: -29.8796475201578\n"
     ]
    }
   ],
   "execution_count": 144
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
