{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-19T15:42:57.399054Z",
     "start_time": "2025-02-19T15:42:47.196071Z"
    }
   },
   "source": [
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pymc as pm\n",
    "import pytensor\n",
    "import pytensor.tensor as pt\n",
    "\n",
    "from pytensor.graph import Apply, Op\n",
    "from scipy.optimize import approx_fprime"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "RANDOM_SEED = 42\n",
    "\n",
    "def stick_breaking(beta):\n",
    "    portion_remaining = pt.concatenate([[1], pt.extra_ops.cumprod(1 - beta)[:-1]])\n",
    "    return beta * portion_remaining\n",
    "\n",
    "def reparameterize(pi):\n",
    "    return pi / pi.sum()\n",
    "\n",
    "def hierarchical_beta(alpha0, beta, nsources, k):\n",
    "    \"\"\"Hierarchical Beta distribution for multiple sources.\"\"\"\n",
    "    pi_tilt_sources = []\n",
    "    \n",
    "    for s in range(nsources):\n",
    "        beta_params = [(alpha0 * beta[k], alpha0 * (1 - pm.math.sum(beta[:k + 1]))) for k in range(k)]\n",
    "        pi_tilt = pm.Beta(f'pi_tilt_{s}', \n",
    "                          alpha=[b[0] for b in beta_params], \n",
    "                          beta=[b[1] for b in beta_params], \n",
    "                          dims=\"component\")\n",
    "        pi_tilt_sources.append(pi_tilt)\n",
    "    \n",
    "    #return pm.math.stack(pi_tilt_sources, axis=0)\n",
    "    return pi_tilt_sources"
   ],
   "id": "1c95b40a68db948a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pytensor.tensor as pt\n",
    "from pytensor.graph.op import Op, Apply\n",
    "import pymc as pm\n",
    "import pandas as pd\n",
    "\n",
    "# Define interpolate function\n",
    "def interpolate(x0, y0, x):\n",
    "    x = np.array(x)\n",
    "    idx = np.searchsorted(x0, x)\n",
    "    dl = np.array(x - x0[idx - 1])\n",
    "    dr = np.array(x0[idx] - x)\n",
    "    d = dl + dr\n",
    "    wl = dr / d\n",
    "    return wl * y0[idx - 1] + (1 - wl) * y0[idx]\n",
    "\n",
    "# Load data and ensure it's a NumPy array\n",
    "data = pd.read_csv('grids_example_1.csv', header=None).values\n",
    "param_min = data[0, :].min()\n",
    "param_max = data[0, :].max()\n",
    "\n",
    "# Function to compute log-likelihood    \n",
    "def my_loglike(x, data):\n",
    "    x_vals = data[0, :]\n",
    "    loglike_vals = data[1, :]\n",
    "    return interpolate(x_vals, loglike_vals, x)\n",
    "\n",
    "# Define custom PyTensor operation\n",
    "class LogLike(Op):\n",
    "    def make_node(self, x, data):\n",
    "        x = pt.as_tensor_variable(x)  # Ensure x is a tensor\n",
    "        data = pt.as_tensor_variable(data)  # Ensure data is a tensor\n",
    "        inputs = [x, data]\n",
    "        outputs = [pt.TensorType(dtype=\"float64\", broadcastable=(False,))()]\n",
    "        return Apply(self, inputs, outputs)\n",
    "\n",
    "    def perform(self, node, inputs, outputs):\n",
    "        x, data = inputs\n",
    "        x_vals = data[0, :]\n",
    "        loglike_vals = data[1, :]\n",
    "        loglike_eval = interpolate(x_vals, loglike_vals, x)\n",
    "        outputs[0][0] = np.array(loglike_eval)\n",
    "\n",
    "# Initialize operation\n",
    "loglike_op = LogLike()\n",
    "\n",
    "# PyMC model without observed data\n",
    "with pm.Model() as no_grad_model:\n",
    "    # Define prior for x\n",
    "    x = pm.Uniform(\"x\", lower=param_min, upper=param_max, shape=1)\n",
    "\n",
    "    # Add a custom potential for the likelihood\n",
    "    pm.Potential(\"likelihood\", loglike_op(x, data))\n",
    "\n",
    "    # Sample posterior\n",
    "    idata = pm.sample(\n",
    "        200000, \n",
    "        tune=5000,\n",
    "        chains=8,\n",
    "    )"
   ],
   "id": "d2eec1ff058c9ae0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pytensor.tensor as pt\n",
    "from pytensor.graph.op import Op, Apply\n",
    "import pymc as pm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define interpolate function\n",
    "def interpolate(x0, y0, x):\n",
    "    x = np.array(x)\n",
    "    idx = np.searchsorted(x0, x)\n",
    "    dl = np.array(x - x0[idx - 1])\n",
    "    dr = np.array(x0[idx] - x)\n",
    "    d = dl + dr\n",
    "    wl = dr / d\n",
    "    return wl * y0[idx - 1] + (1 - wl) * y0[idx]\n",
    "\n",
    "# Load data and ensure it's a NumPy array\n",
    "data = pd.read_csv('grids_example_1.csv', header=None).values\n",
    "param_min = data[0, :].min()\n",
    "param_max = data[0, :].max()"
   ],
   "id": "63627e3544dc7fcf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# plot the data\n",
    "plt.plot(data[0, :], data[1, :], 'o')\n",
    "\n",
    "plt.show()"
   ],
   "id": "f772f4f7da646e72",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **1. Interpolation of Log-Likelihoods**\n",
    "For each outcome \\( j \\), we interpolate the log-likelihood for each component \\( k \\):\n",
    "\n",
    "$$\\hat{\\ell}_{j,k} = \\text{interpolate}(x_{\\text{grid},j}, \\ell_{\\text{grid},j}, \\beta_{j,k})$$\n",
    "\n",
    "\n",
    "$$\\ell_k = \\sum_{j=1}^{N_{\\text{outcomes}}} \\hat{\\ell}_{j,k}$$\n",
    "\n",
    "$$\\ell_{\\text{final}} = \\log \\sum_{k=1}^{N_{\\text{components}}} \\pi_k e^{\\ell_k}$$\n"
   ],
   "id": "c38e89b51668f407"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T15:44:17.296129Z",
     "start_time": "2025-02-19T15:42:57.408757Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytensor.tensor as pt\n",
    "from pytensor.graph.op import Op, Apply\n",
    "import pymc as pm\n",
    "\n",
    "# Load profile likelihood data\n",
    "data = pd.read_csv(\"profileLikelihoods_NCs_long.csv\")\n",
    "\n",
    "# Define interpolation function\n",
    "def interpolate(x0, y0, x):\n",
    "    x = np.array(x)\n",
    "    idx = np.searchsorted(x0, x)\n",
    "    dl = np.array(x - x0[idx - 1])\n",
    "    dr = np.array(x0[idx] - x)\n",
    "    d = dl + dr\n",
    "    wl = dr / d\n",
    "    return wl * y0[idx - 1] + (1 - wl) * y0[idx]\n",
    "\n",
    "# Define custom likelihood function\n",
    "class LogLike(Op):\n",
    "    def make_node(self, β, num_outcomes, source_data):\n",
    "        β = pt.as_tensor(β)  # Ensure scalar\n",
    "        num_outcomes = pt.as_tensor(int(num_outcomes))  # Ensure scalar integer\n",
    "        source_data = pt.as_tensor(np.asarray(source_data))  # Ensure NumPy array\n",
    "\n",
    "        # The output must be a single scalar\n",
    "        outputs = [pt.dscalar()]\n",
    "        return Apply(self, [β, num_outcomes, source_data], outputs)\n",
    "\n",
    "    def perform(self, node, inputs, outputs):\n",
    "        β, num_outcomes, source_data = inputs\n",
    "        num_outcomes = int(num_outcomes)  # Ensure integer\n",
    "\n",
    "        # Compute total log-likelihood (scalar β)\n",
    "        total_log_likelihood = 0.0\n",
    "\n",
    "        for j in range(num_outcomes):\n",
    "            # Extract outcome-specific x_vals and loglike_vals\n",
    "            outcome_data = source_data[source_data[:, 2] == (j+1)]  # Filter by outcome index\n",
    "            \n",
    "            if outcome_data.shape[0] == 0:\n",
    "                raise ValueError(f\"No data found for outcome {j+1} in source_data!\")\n",
    "\n",
    "            x_vals = outcome_data[:, 0]  # Parameter grid points\n",
    "            loglike_vals = outcome_data[:, 1]  # Log-likelihood values\n",
    "\n",
    "            # Interpolate the log-likelihood for β\n",
    "            loglike_j = interpolate(x_vals, loglike_vals, β)\n",
    "\n",
    "            # Sum in log-space\n",
    "            total_log_likelihood += loglike_j\n",
    "\n",
    "        # Ensure the output is a scalar\n",
    "        outputs[0][0] = np.array(total_log_likelihood)\n",
    "\n",
    "# Initialize the custom operation\n",
    "loglike_op = LogLike()\n",
    "\n",
    "# Simulated test case for source 1\n",
    "n_outcomes_test = int(data[\"outcome\"].nunique())  # Ensure integer\n",
    "n_components = 3  # Assume 3 components for illustration\n",
    "\n",
    "np.random.seed(42)\n",
    "β_test = np.random.uniform(low=-1, high=-0.5)  # Simulated β values\n",
    "num_outcomes_test = int(data[data[\"source\"] == 1][\"outcome\"].nunique())  # Ensure integer\n",
    "source_1_data = data[data[\"source\"] == 1][[\"point\", \"value\", \"outcome\"]].values  # Include outcome column\n",
    "weights_test = np.full(n_components, 1 / n_components)  # Equal mixture weights\n",
    "\n",
    "\n",
    "test_out = loglike_op(β_test, num_outcomes_test, source_1_data)\n",
    "print(\"Evaluated Test Likelihood Output:\", test_out.eval())\n",
    "\n",
    "\n"
   ],
   "id": "385ca4107aaf8b2c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated Test Likelihood Output: -30.351530816420613\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T04:59:21.748832Z",
     "start_time": "2025-02-19T04:59:21.743011Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.random.seed(42)\n",
    "β_test = np.random.uniform(low=-1, high=-0.5)  # Simulated β values\n",
    "num_outcomes_test = int(data[data[\"source\"] == 1][\"outcome\"].nunique())  # Ensure integer\n",
    "source_1_data = data[data[\"source\"] == 1][[\"point\", \"value\", \"outcome\"]].values  # Include outcome column\n",
    "weights_test = np.full(n_components, 1 / n_components)  # Equal mixture weights"
   ],
   "id": "bf6cc4850c9fda36",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Likelihood with gradient",
   "id": "901f1dc0dd20b60c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T16:23:13.059041Z",
     "start_time": "2025-02-19T16:23:13.047601Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def my_loglike(β, num_outcomes, source_data):\n",
    "    # We fail explicitly if inputs are not numerical types for the sake of this tutorial\n",
    "    # As defined, my_loglike would actually work fine with PyTensor variables!\n",
    "    for param in (β, num_outcomes, source_data):\n",
    "        if not isinstance(param, (float, int, np.ndarray)):\n",
    "            raise TypeError(f\"Invalid input type to loglike: {type(param)}\")\n",
    "            β, num_outcomes, source_data = inputs\n",
    "    num_outcomes = int(num_outcomes)  # Ensure integer\n",
    "\n",
    "    # Compute total log-likelihood (scalar β)\n",
    "    total_log_likelihood = 0.0\n",
    "\n",
    "    for j in range(num_outcomes):\n",
    "        # Extract outcome-specific x_vals and loglike_vals\n",
    "        outcome_data = source_data[source_data[:, 2] == (j+1)]  # Filter by outcome index\n",
    "\n",
    "        if outcome_data.shape[0] == 0:\n",
    "            raise ValueError(f\"No data found for outcome {j+1} in source_data!\")\n",
    "\n",
    "        x_vals = outcome_data[:, 0]  # Parameter grid points\n",
    "        loglike_vals = outcome_data[:, 1]  # Log-likelihood values\n",
    "\n",
    "        # Interpolate the log-likelihood for β\n",
    "        loglike_j = interpolate(x_vals, loglike_vals, β)\n",
    "\n",
    "        # Sum in log-space\n",
    "        total_log_likelihood += loglike_j\n",
    "\n",
    "    # Ensure the output is a scalar\n",
    "    return np.array(total_log_likelihood)"
   ],
   "id": "879ae4a158fd1b48",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T16:23:14.557181Z",
     "start_time": "2025-02-19T16:23:14.540614Z"
    }
   },
   "cell_type": "code",
   "source": "my_loglike(β_test, num_outcomes_test, source_1_data)",
   "id": "e7ac060aa3fc4f9d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(-30.35153082)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T17:22:51.038650Z",
     "start_time": "2025-02-19T17:22:51.019620Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def finite_differences_loglike(β, num_outcomes, source_data, eps=1e-5):\n",
    "    β = np.atleast_1d(β)  # Ensure `β` is a 1D array\n",
    "    \n",
    "    def inner_func(β):\n",
    "        return np.array([my_loglike(β[0], num_outcomes, source_data)])  # Ensure array output\n",
    "\n",
    "    grad_wrt_β = approx_fprime(β, inner_func, [eps])  # Compute gradient as array\n",
    "    return grad_wrt_β[0]  # Return the first element as scalar"
   ],
   "id": "9f11269519125ab9",
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T17:22:51.540443Z",
     "start_time": "2025-02-19T17:22:51.521846Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define interpolation function\n",
    "def interpolate(x0, y0, x):\n",
    "    x = np.array(x)\n",
    "\n",
    "    if x0.size == 0 or y0.size == 0:\n",
    "        raise ValueError(\"Empty data passed to interpolation!\")\n",
    "\n",
    "    idx = np.searchsorted(x0, x) - 1  # Ensure index is within bounds\n",
    "    idx = np.clip(idx, 0, len(x0) - 2)  # Clip to avoid out-of-range errors\n",
    "\n",
    "    dl = x - x0[idx]\n",
    "    dr = x0[idx + 1] - x\n",
    "    d = dl + dr\n",
    "\n",
    "    # Prevent division by zero\n",
    "    wl = np.where(d != 0, dr / d, 0.5)\n",
    "    return wl * y0[idx] + (1 - wl) * y0[idx + 1]\n",
    "\n",
    "# Define `LogLike` with gradients\n",
    "class LogLike(Op):\n",
    "    def make_node(self, β, num_outcomes, source_data):\n",
    "        β = pt.as_tensor(β)  # Ensure scalar\n",
    "        num_outcomes = pt.as_tensor(num_outcomes, dtype=\"int32\")  # Ensure scalar integer\n",
    "        source_data = pt.as_tensor(source_data, dtype=np.float64)  # Ensure NumPy array\n",
    "\n",
    "        # The output must be a single scalar\n",
    "        outputs = [pt.dscalar()]\n",
    "        return Apply(self, [β, num_outcomes, source_data], outputs)\n",
    "\n",
    "    def perform(self, node, inputs, outputs):\n",
    "        β, num_outcomes, source_data = inputs\n",
    "        num_outcomes = int(num_outcomes)  # Ensure integer\n",
    "\n",
    "        # Compute total log-likelihood (scalar β)\n",
    "        total_log_likelihood = 0.0\n",
    "\n",
    "        for j in range(num_outcomes):\n",
    "            # Extract outcome-specific x_vals and loglike_vals\n",
    "            outcome_data = source_data[source_data[:, 2] == (j+1)]  # Filter by outcome index\n",
    "\n",
    "            if outcome_data.shape[0] == 0:\n",
    "                raise ValueError(f\"No data found for outcome {j+1} in source_data!\")\n",
    "\n",
    "            x_vals = outcome_data[:, 0]  # Parameter grid points\n",
    "            loglike_vals = outcome_data[:, 1]  # Log-likelihood values\n",
    "\n",
    "            # Interpolate the log-likelihood for β\n",
    "            loglike_j = interpolate(x_vals, loglike_vals, β)\n",
    "\n",
    "            # Sum in log-space\n",
    "            total_log_likelihood += loglike_j\n",
    "\n",
    "        # Ensure the output is a scalar\n",
    "        outputs[0][0] = np.array(total_log_likelihood)\n",
    "\n",
    "    def grad(self, inputs: list[pt.TensorVariable], g: list[pt.TensorVariable])-> list[pt.TensorVariable]:\n",
    "        β, num_outcomes, source_data = inputs\n",
    "\n",
    "        # Compute gradient using finite differences\n",
    "        grad_β = loglikegrad_op(β, num_outcomes, source_data)\n",
    "\n",
    "        [out_grad] = g\n",
    "        return [\n",
    "            pt.sum(out_grad * grad_β),  # Gradient w.r.t. β\n",
    "            pytensor.gradient.grad_not_implemented(self, 1, num_outcomes),  # No gradient w.r.t. num_outcomes\n",
    "            pytensor.gradient.grad_not_implemented(self, 2, source_data),   # No gradient w.r.t. source_data\n",
    "        ]\n",
    "\n",
    "# Define the gradient computation using finite differences\n",
    "class LogLikeGrad(Op):\n",
    "    def make_node(self, β, num_outcomes, source_data):\n",
    "        β = pt.as_tensor(β)\n",
    "        num_outcomes = pt.as_tensor(num_outcomes, dtype=\"int32\") \n",
    "        source_data = pt.as_tensor(source_data, dtype=np.float64) \n",
    "\n",
    "        inputs = [β, num_outcomes, source_data]\n",
    "        outputs = [β.type()]  # Output shape matches β\n",
    "\n",
    "        return Apply(self, inputs, outputs)\n",
    "\n",
    "    def perform(self, node, inputs, outputs):\n",
    "        β, num_outcomes, source_data = inputs\n",
    "        grad_β = finite_differences_loglike(β, num_outcomes, source_data)\n",
    "        outputs[0][0] = np.array(grad_β)\n",
    "\n",
    "# Initialize the Ops\n",
    "loglike_op = LogLike()\n",
    "loglikegrad_op = LogLikeGrad()"
   ],
   "id": "ba2a358f38f7ebc5",
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T17:22:52.092631Z",
     "start_time": "2025-02-19T17:22:52.072814Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pytensor.tensor as pt\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate test data for β\n",
    "β = pt.scalar(\"β\")\n",
    "num_outcomes_test = int(data[data[\"source\"] == 1][\"outcome\"].nunique())  # Ensure integer\n",
    "source_1_data = data[data[\"source\"] == 1][[\"point\", \"value\", \"outcome\"]].astype(float).values  # Convert to float\n",
    "\n",
    "# Compute likelihood\n",
    "loglike_output = loglike_op(β, num_outcomes_test, source_1_data)\n",
    "eval_out = loglike_output.eval({β: -0.5})\n",
    "print(eval_out)"
   ],
   "id": "771dc15f89a11041",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-27.66955915202564\n"
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T17:25:01.629036Z",
     "start_time": "2025-02-19T17:25:01.610139Z"
    }
   },
   "cell_type": "code",
   "source": [
    "grad_b = pytensor.grad(loglike_output.sum(), wrt=[β])\n",
    "pytensor.dprint(grad_b, print_type=True)\n"
   ],
   "id": "b7e465a3688e3ec5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum{axes=None} [id A] <Scalar(float64, shape=())>\n",
      " └─ Mul [id B] <Scalar(float64, shape=())>\n",
      "    ├─ Second [id C] <Scalar(float64, shape=())>\n",
      "    │  ├─ LogLike [id D] <Scalar(float64, shape=())>\n",
      "    │  │  ├─ β [id E] <Scalar(float64, shape=())>\n",
      "    │  │  ├─ 6 [id F] <Scalar(int32, shape=())>\n",
      "    │  │  └─ [[ -2.3025 ... 6.     ]] [id G] <Matrix(float64, shape=(66, 3))>\n",
      "    │  └─ DimShuffle{order=[]} [id H] <Scalar(float64, shape=())>\n",
      "    │     └─ Second [id I] <Scalar(float64, shape=())>\n",
      "    │        ├─ Sum{axes=None} [id J] <Scalar(float64, shape=())>\n",
      "    │        │  └─ LogLike [id D] <Scalar(float64, shape=())>\n",
      "    │        │     └─ ···\n",
      "    │        └─ 1.0 [id K] <Scalar(float64, shape=())>\n",
      "    └─ LogLikeGrad [id L] <Scalar(float64, shape=())>\n",
      "       ├─ β [id E] <Scalar(float64, shape=())>\n",
      "       ├─ 6 [id F] <Scalar(int32, shape=())>\n",
      "       └─ [[ -2.3025 ... 6.     ]] [id G] <Matrix(float64, shape=(66, 3))>\n"
     ]
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T17:25:02.531861Z",
     "start_time": "2025-02-19T17:25:02.520477Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def stick_breaking(beta):\n",
    "    portion_remaining = pt.concatenate([[1], pt.extra_ops.cumprod(1 - beta)[:-1]])\n",
    "    return beta * portion_remaining\n",
    "\n",
    "def reparameterize(pi):\n",
    "    return pi / pi.sum()\n",
    "\n",
    "def hierarchical_beta(alpha0, beta, nsources, k):\n",
    "    \"\"\"Hierarchical Beta distribution for multiple sources.\"\"\"\n",
    "    pi_tilt_sources = []\n",
    "    \n",
    "    for s in range(nsources):\n",
    "        beta_params = [(alpha0 * beta[k], alpha0 * (1 - pm.math.sum(beta[:k + 1]))) for k in range(k)]\n",
    "        pi_tilt = pm.Beta(f'pi_tilt_{s}', \n",
    "                          alpha=[b[0] for b in beta_params], \n",
    "                          beta=[b[1] for b in beta_params], \n",
    "                          dims=\"component\")\n",
    "        pi_tilt_sources.append(pi_tilt)\n",
    "    \n",
    "    #return pm.math.stack(pi_tilt_sources, axis=0)\n",
    "    return pi_tilt_sources"
   ],
   "id": "d12ff7b9b82240cf",
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-02-19T17:25:03.143535Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pymc as pm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load profile likelihood data\n",
    "data = pd.read_csv(\"profileLikelihoods_NCs_long.csv\")\n",
    "\n",
    "# Constants\n",
    "N_sources = data[\"source\"].nunique()  # Number of sources\n",
    "k = 5  # Number of mixture components\n",
    "\n",
    "# Precompute source data and number of outcomes\n",
    "source_data_dict = {}\n",
    "num_outcomes_dict = {}\n",
    "\n",
    "for s in range(1, N_sources + 1):  # Ensure correct indexing\n",
    "    source_data_dict[s] = data[data[\"source\"] == s][[\"point\", \"value\", \"outcome\"]].astype(float).values\n",
    "    num_outcomes_dict[s] = int(data[data[\"source\"] == s][\"outcome\"].nunique())\n",
    "\n",
    "# PyMC Model\n",
    "with pm.Model(coords={\"component\": np.arange(k), \"n_source\": np.arange(N_sources)}) as HDP_model:\n",
    "\n",
    "    # Priors for hierarchical Dirichlet process weights\n",
    "    gamma = pm.Gamma(\"gamma\", 1.0, 5.0)\n",
    "    alpha0 = pm.Gamma(\"alpha0\", 1.0, 5.0)\n",
    "    \n",
    "    # Stick-breaking process for mixture weights\n",
    "    beta_tilt = pm.Beta(\"beta_tilt\", 1.0, gamma, dims=\"component\")\n",
    "    beta = pm.Deterministic(\"beta\", stick_breaking(beta_tilt), dims=\"component\")\n",
    "    π_tilt = hierarchical_beta(alpha0, beta, nsources=N_sources, k=k)\n",
    "\n",
    "    π_norms = []\n",
    "    for s in range(N_sources):\n",
    "        π = pm.Deterministic(f\"π_{s}\", stick_breaking(π_tilt[s]), dims=\"component\")\n",
    "        π_norms.append(pm.Deterministic(f\"π_norm_{s}\", reparameterize(π), dims=[\"component\"]))\n",
    "\n",
    "    # Mixture component parameters\n",
    "    μ = pm.Normal(\n",
    "        \"μ\",\n",
    "        mu=data[\"point\"].mean(),  # Use the mean of the profile likelihood grid points\n",
    "        sigma=10,\n",
    "        shape=k,\n",
    "        transform=pm.distributions.transforms.ordered,  # Ensure μ is ordered\n",
    "        initval=np.linspace(-1, 1, k),  # Reasonable ordered starting values\n",
    "    )\n",
    "\n",
    "    σ = pm.HalfNormal(\"σ\", sigma=10, shape=k)\n",
    "\n",
    "    # Likelihood for each source\n",
    "    for s in range(1, N_sources + 1):  # Ensure correct indexing\n",
    "        # Draw β from the mixture model\n",
    "        β = pm.NormalMixture(f'β_{s}', w=π_norms[s-1], mu=μ, sigma=σ)\n",
    "        βt = pm.Deterministic(f\"β_clipped_{s}\", pm.math.clip(β, source_data_dict[s][:, 0].min(), source_data_dict[s][:, 0].max()))\n",
    "        #βt = pm.Truncated(f\"β_truncated_{s}\", β , lower=source_data[:, 0].min(), upper=source_data[:, 0].max())\n",
    "        # Use precomputed values\n",
    "        num_outcomes = num_outcomes_dict[s]\n",
    "        source_data = source_data_dict[s]\n",
    "\n",
    "        likelihood = pm.Potential(f\"loglike_{s}\", loglike_op(βt, num_outcomes, source_data))\n",
    "\n",
    "    # Sampling\n",
    "    trace = pm.sample(\n",
    "        tune=5000,\n",
    "        draws=20000,\n",
    "        init=\"advi\",\n",
    "        #step=pm.Metropolis(),  # Use Metropolis for non-differentiable likelihood\n",
    "        random_seed=42,\n",
    "    )\n",
    "\n"
   ],
   "id": "7fec6f99099033a6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using advi...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Output()"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "52caf22dd98e41deac11bb6dbf63b86c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ca3fc04b92932993"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
