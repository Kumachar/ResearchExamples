{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-01T04:41:44.864361Z",
     "start_time": "2025-04-01T04:23:51.265193Z"
    }
   },
   "source": [
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpyro\n",
    "import numpyro.distributions as dist\n",
    "from numpyro.infer import MCMC, NUTS\n",
    "\n",
    "data = pd.read_csv(\"profileLikelihoods_NCs_long.csv\")\n",
    "\n",
    "\n",
    "N_sources = 8\n",
    "\n",
    "source_outcome_data = {}\n",
    "for s in range(1, N_sources + 1):\n",
    "    df = data[data[\"source\"] == s][[\"point\", \"value\", \"outcome\"]].astype(float)\n",
    "    outcome_list = []\n",
    "\n",
    "    for outcome in sorted(df[\"outcome\"].unique()):\n",
    "        outcome_array = jnp.array(df[df[\"outcome\"] == outcome].values)\n",
    "        outcome_list.append(outcome_array)\n",
    "    source_outcome_data[s] = outcome_list\n",
    "\n",
    "\n",
    "num_outcomes_dict = {s: len(source_outcome_data[s]) for s in range(1, N_sources + 1)}\n",
    "\n",
    "\n",
    "def interpolate(x0, y0, x):\n",
    "    \"\"\"Linear interpolation using JAX's jnp.interp.\"\"\"\n",
    "    return jnp.interp(x, x0, y0)\n",
    "\n",
    "def custom_loglike(beta, outcome_data_list):\n",
    "    \"\"\"\n",
    "    Computes the total log-likelihood for a given beta value.\n",
    "    outcome_data_list is a list of preprocessed arrays, one for each outcome.\n",
    "    \"\"\"\n",
    "    total_ll = 0.0\n",
    "    for outcome_data in outcome_data_list:\n",
    "        x_vals = outcome_data[:, 0]       # grid points\n",
    "        loglike_vals = outcome_data[:, 1]   # corresponding log-likelihood values\n",
    "        ll = jnp.interp(beta, x_vals, loglike_vals)\n",
    "        total_ll += ll\n",
    "    return total_ll\n",
    "\n",
    "def stick_breaking(beta):\n",
    "    \"\"\"Perform stick-breaking transformation.\"\"\"\n",
    "    portion_remaining = jnp.concatenate([jnp.array([1.0]), jnp.cumprod(1 - beta)[:-1]])\n",
    "    return beta * portion_remaining\n",
    "\n",
    "def reparameterize(pi):\n",
    "    \"\"\"Normalize to sum to one.\"\"\"\n",
    "    return pi / jnp.sum(pi)\n",
    "\n",
    "# -------------------------\n",
    "# NumPyro Model Definition\n",
    "# -------------------------\n",
    "def HDP_model(source_outcome_data, num_outcomes_dict, N_sources, k, data_point_mean):\n",
    "    # Hyperpriors\n",
    "    gamma = numpyro.sample(\"gamma\", dist.Gamma(1.0, 5.0))\n",
    "    alpha0 = numpyro.sample(\"alpha0\", dist.Gamma(1.0, 5.0))\n",
    "    \n",
    "    # Global stick-breaking for mixture weights.\n",
    "    beta_tilt = numpyro.sample(\"beta_tilt\", dist.Beta(1.0, gamma).expand([k]))\n",
    "    beta = stick_breaking(beta_tilt)\n",
    "    \n",
    "    beta_cumsum = jnp.cumsum(beta)\n",
    "    pi_tilt = numpyro.sample(\"pi_tilt\",\n",
    "                             dist.Beta(alpha0 * beta, alpha0 * (1 - beta_cumsum))\n",
    "                             .expand([N_sources, k]))\n",
    "    \n",
    "    # Compute source-specific mixture weights\n",
    "    pi_norms = []\n",
    "    for s in range(N_sources):\n",
    "        pi_source = stick_breaking(pi_tilt[s])\n",
    "        pi_norm = reparameterize(pi_source)\n",
    "        pi_norms.append(pi_norm)\n",
    "    \n",
    "    # Mixture component parameters (μ not forced to be ordered here)\n",
    "    mu = numpyro.sample(\"mu\", dist.Normal(data_point_mean, 10.0).expand([k]))\n",
    "    sigma = numpyro.sample(\"sigma\", dist.HalfNormal(10.0).expand([k]))\n",
    "    \n",
    "    # Likelihood for each source\n",
    "    for s in range(1, N_sources + 1):\n",
    "        # Define a mixture model for source s using the precomputed weights.\n",
    "        mixture_dist = dist.MixtureSameFamily(\n",
    "            dist.Categorical(probs=pi_norms[s - 1]),\n",
    "            dist.Normal(loc=mu, scale=sigma)\n",
    "        )\n",
    "        # Sample beta for source s\n",
    "        beta_s = numpyro.sample(f\"beta_{s}\", mixture_dist)\n",
    "        \n",
    "        # Use the pre-processed outcome-specific arrays for source s.\n",
    "        outcome_data_list = source_outcome_data[s]\n",
    "        # Define clipping bounds based on the grid points (using first and last outcome arrays).\n",
    "        grid_min = jnp.min(outcome_data_list[0][:, 0])\n",
    "        grid_max = jnp.max(outcome_data_list[-1][:, 0])\n",
    "        beta_clipped = jnp.clip(beta_s, grid_min, grid_max)\n",
    "        \n",
    "        # Add the custom log-likelihood as an extra factor.\n",
    "        # This factor effectively incorporates your posterior likelihood.\n",
    "        ll = custom_loglike(beta_clipped, outcome_data_list)\n",
    "        numpyro.factor(f\"loglike_{s}\", ll)\n",
    "\n",
    "# -------------------------\n",
    "# Inference\n",
    "# -------------------------\n",
    "# Create a PRNG key\n",
    "rng_key = random.PRNGKey(0)\n",
    "data_point_mean = data[\"point\"].mean()\n",
    "k = 5  # Number of mixture components\n",
    "\n",
    "# Set up and run MCMC using the NUTS sampler.\n",
    "nuts_kernel = NUTS(HDP_model)\n",
    "mcmc = MCMC(nuts_kernel, num_warmup=5000, num_samples=20000)\n",
    "mcmc.run(rng_key,\n",
    "         source_outcome_data=source_outcome_data,\n",
    "         num_outcomes_dict=num_outcomes_dict,\n",
    "         N_sources=N_sources,\n",
    "         k=k,\n",
    "         data_point_mean=data_point_mean)\n",
    "mcmc.print_summary()\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|██████████| 25000/25000 [17:52<00:00, 23.31it/s, 127 steps of size 2.84e-02. acc. prob=0.86]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                  mean       std    median      5.0%     95.0%     n_eff     r_hat\n",
      "      alpha0      0.94      0.34      0.89      0.42      1.45   2006.68      1.00\n",
      "      beta_1     11.12      8.91      8.51      1.41     23.01   1836.90      1.00\n",
      "      beta_2      1.54      0.26      1.56      1.16      1.94   1341.71      1.00\n",
      "      beta_3      0.77      0.10      0.75      0.63      0.93   1653.27      1.00\n",
      "      beta_4      0.77      0.07      0.77      0.67      0.88   2187.29      1.00\n",
      "      beta_5      0.71      0.04      0.71      0.64      0.78   4680.01      1.00\n",
      "      beta_6      0.68      0.04      0.68      0.62      0.74   5383.31      1.00\n",
      "      beta_7      0.67      0.03      0.67      0.61      0.72   4823.89      1.00\n",
      "      beta_8      0.67      0.03      0.67      0.62      0.71   3809.55      1.00\n",
      "beta_tilt[0]      0.46      0.13      0.47      0.26      0.67   1948.82      1.00\n",
      "beta_tilt[1]      0.22      0.13      0.19      0.04      0.41   1445.47      1.00\n",
      "beta_tilt[2]      0.25      0.13      0.22      0.05      0.44   1502.39      1.00\n",
      "beta_tilt[3]      0.29      0.14      0.27      0.07      0.51   2282.08      1.00\n",
      "beta_tilt[4]      0.35      0.15      0.33      0.10      0.59   2313.48      1.00\n",
      "       gamma      0.86      0.36      0.81      0.30      1.40   6567.60      1.00\n",
      "       mu[0]      0.72      0.09      0.70      0.62      0.83    605.87      1.00\n",
      "       mu[1]      2.29      8.90      2.33    -12.49     17.54   5211.66      1.00\n",
      "       mu[2]      2.23      8.92      2.28    -13.21     16.47   4208.67      1.00\n",
      "       mu[3]      2.13      9.16      2.15    -13.36     17.23   4600.62      1.00\n",
      "       mu[4]      1.87      9.27      1.92    -13.32     17.67   4687.52      1.00\n",
      "pi_tilt[0,0]      0.21      0.25      0.10      0.00      0.61   6122.04      1.00\n",
      "pi_tilt[0,1]      0.24      0.36      0.01      0.00      0.95   3434.34      1.00\n",
      "pi_tilt[0,2]      0.27      0.39      0.01      0.00      0.99   3993.78      1.00\n",
      "pi_tilt[0,3]      0.32      0.42      0.02      0.00      1.00   4653.76      1.00\n",
      "pi_tilt[0,4]      0.36      0.44      0.03      0.00      1.00   4729.00      1.00\n",
      "pi_tilt[1,0]      0.28      0.31      0.14      0.00      0.82   1751.03      1.00\n",
      "pi_tilt[1,1]      0.24      0.36      0.01      0.00      0.94   3064.73      1.00\n",
      "pi_tilt[1,2]      0.28      0.39      0.01      0.00      0.99   3662.19      1.00\n",
      "pi_tilt[1,3]      0.31      0.41      0.01      0.00      1.00   4713.76      1.00\n",
      "pi_tilt[1,4]      0.34      0.43      0.02      0.00      1.00   4715.50      1.00\n",
      "pi_tilt[2,0]      0.66      0.31      0.76      0.15      1.00   4474.50      1.00\n",
      "pi_tilt[2,1]      0.19      0.33      0.00      0.00      0.88   5893.57      1.00\n",
      "pi_tilt[2,2]      0.22      0.36      0.00      0.00      0.96   5264.58      1.00\n",
      "pi_tilt[2,3]      0.24      0.38      0.00      0.00      0.99   6527.38      1.00\n",
      "pi_tilt[2,4]      0.27      0.41      0.00      0.00      1.00   6242.05      1.00\n",
      "pi_tilt[3,0]      0.66      0.32      0.76      0.12      1.00   3083.60      1.00\n",
      "pi_tilt[3,1]      0.19      0.33      0.00      0.00      0.88   5633.26      1.00\n",
      "pi_tilt[3,2]      0.23      0.36      0.00      0.00      0.97   4414.65      1.00\n",
      "pi_tilt[3,3]      0.24      0.38      0.00      0.00      0.99   6624.52      1.00\n",
      "pi_tilt[3,4]      0.27      0.41      0.00      0.00      1.00   6936.46      1.00\n",
      "pi_tilt[4,0]      0.68      0.31      0.78      0.17      1.00   5049.73      1.00\n",
      "pi_tilt[4,1]      0.19      0.33      0.00      0.00      0.87   7132.15      1.00\n",
      "pi_tilt[4,2]      0.22      0.36      0.00      0.00      0.96   6651.42      1.00\n",
      "pi_tilt[4,3]      0.24      0.38      0.00      0.00      0.99   7548.52      1.00\n",
      "pi_tilt[4,4]      0.27      0.41      0.00      0.00      1.00   5592.55      1.00\n",
      "pi_tilt[5,0]      0.67      0.31      0.77      0.16      1.00   4888.27      1.00\n",
      "pi_tilt[5,1]      0.19      0.33      0.00      0.00      0.88   6681.76      1.00\n",
      "pi_tilt[5,2]      0.22      0.36      0.00      0.00      0.96   6109.33      1.00\n",
      "pi_tilt[5,3]      0.24      0.38      0.00      0.00      0.99   7001.63      1.00\n",
      "pi_tilt[5,4]      0.28      0.41      0.00      0.00      1.00   6540.15      1.00\n",
      "pi_tilt[6,0]      0.67      0.31      0.77      0.16      1.00   4631.61      1.00\n",
      "pi_tilt[6,1]      0.19      0.33      0.00      0.00      0.87   5720.42      1.00\n",
      "pi_tilt[6,2]      0.22      0.36      0.00      0.00      0.95   5955.31      1.00\n",
      "pi_tilt[6,3]      0.24      0.38      0.00      0.00      0.99   8655.52      1.00\n",
      "pi_tilt[6,4]      0.27      0.40      0.00      0.00      1.00   7584.92      1.00\n",
      "pi_tilt[7,0]      0.67      0.31      0.76      0.17      1.00   4845.50      1.00\n",
      "pi_tilt[7,1]      0.20      0.33      0.00      0.00      0.89   5349.22      1.00\n",
      "pi_tilt[7,2]      0.21      0.35      0.00      0.00      0.95   6636.99      1.00\n",
      "pi_tilt[7,3]      0.25      0.38      0.00      0.00      0.99   7739.61      1.00\n",
      "pi_tilt[7,4]      0.27      0.40      0.00      0.00      1.00   5167.50      1.00\n",
      "    sigma[0]      0.14      0.69      0.07      0.00      0.26    621.65      1.00\n",
      "    sigma[1]      7.77      6.02      6.47      0.00     16.09   3818.73      1.00\n",
      "    sigma[2]      7.63      5.82      6.38      0.04     15.90   5615.39      1.00\n",
      "    sigma[3]      7.62      5.82      6.39      0.00     15.77   6723.98      1.00\n",
      "    sigma[4]      7.89      6.01      6.59      0.00     16.36   5318.63      1.00\n",
      "\n",
      "Number of divergences: 8831\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T04:23:51.221378Z",
     "start_time": "2025-04-01T02:10:15.688821Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpyro\n",
    "import numpyro.distributions as dist\n",
    "from numpyro.infer import MCMC, NUTS\n",
    "\n",
    "data = pd.read_csv(\"profileLikelihoods_NCs_long.csv\")\n",
    "\n",
    "N_sources = 8\n",
    "\n",
    "source_outcome_data = {}\n",
    "for s in range(1, N_sources + 1):\n",
    "    df = data[data[\"source\"] == s][[\"point\", \"value\", \"outcome\"]].astype(float)\n",
    "    outcome_list = []\n",
    "    for outcome in sorted(df[\"outcome\"].unique()):\n",
    "        outcome_array = jnp.array(df[df[\"outcome\"] == outcome].values)\n",
    "        outcome_list.append(outcome_array)\n",
    "    source_outcome_data[s] = outcome_list\n",
    "\n",
    "num_outcomes_dict = {s: len(source_outcome_data[s]) for s in range(1, N_sources + 1)}\n",
    "\n",
    "def interpolate(x0, y0, x):\n",
    "    \"\"\"Linear interpolation using JAX's jnp.interp.\"\"\"\n",
    "    return jnp.interp(x, x0, y0)\n",
    "\n",
    "def custom_loglike(beta, outcome_data_list):\n",
    "    \"\"\"\n",
    "    Computes the total log-likelihood for a given beta value.\n",
    "    outcome_data_list is a list of preprocessed arrays, one for each outcome.\n",
    "    \"\"\"\n",
    "    total_ll = 0.0\n",
    "    for outcome_data in outcome_data_list:\n",
    "        # Extract grid points and corresponding log-likelihood values.\n",
    "        x_vals = outcome_data[:, 0]       # grid points\n",
    "        loglike_vals = outcome_data[:, 1]   # corresponding log-likelihood values\n",
    "        # Interpolate log-likelihood at the given beta value.\n",
    "        ll = jnp.interp(beta, x_vals, loglike_vals)\n",
    "        total_ll += ll\n",
    "    return total_ll\n",
    "\n",
    "def stick_breaking(beta):\n",
    "    \"\"\"Perform stick-breaking transformation.\"\"\"\n",
    "    portion_remaining = jnp.concatenate([jnp.array([1.0]), jnp.cumprod(1 - beta)[:-1]])\n",
    "    return beta * portion_remaining\n",
    "\n",
    "def reparameterize(pi):\n",
    "    \"\"\"Normalize to sum to one.\"\"\"\n",
    "    return pi / jnp.sum(pi)\n",
    "\n",
    "# -------------------------\n",
    "# NumPyro Model Definition\n",
    "# -------------------------\n",
    "def HDP_model(source_outcome_data, num_outcomes_dict, N_sources, k, data_point_mean):\n",
    "    # Hyperpriors\n",
    "    pi_global = numpyro.sample(\"pi_global\", dist.Dirichlet(jnp.ones(k)))\n",
    "    \n",
    "    # Mixture component parameters (μ not forced to be ordered here)\n",
    "    mu = numpyro.sample(\"mu\", dist.Normal(data_point_mean, 10.0).expand([k]))\n",
    "    sigma = numpyro.sample(\"sigma\", dist.HalfNormal(10.0).expand([k]))\n",
    "    \n",
    "    # Likelihood for each source using the same global mixture proportions.\n",
    "    for s in range(1, N_sources + 1):\n",
    "        mixture_dist = dist.MixtureSameFamily(\n",
    "            dist.Categorical(probs=pi_global),\n",
    "            dist.Normal(loc=mu, scale=sigma)\n",
    "        )\n",
    "        # Sample beta for source s from the common mixture model.\n",
    "        beta_s = numpyro.sample(f\"beta_{s}\", mixture_dist)\n",
    "        \n",
    "        # Use the pre-processed outcome-specific arrays for source s.\n",
    "        outcome_data_list = source_outcome_data[s]\n",
    "        # Define clipping bounds based on the grid points (using first and last outcome arrays).\n",
    "        grid_min = jnp.min(outcome_data_list[0][:, 0])\n",
    "        grid_max = jnp.max(outcome_data_list[-1][:, 0])\n",
    "        beta_clipped = jnp.clip(beta_s, grid_min, grid_max)\n",
    "        \n",
    "        # Add the custom log-likelihood as an extra factor.\n",
    "        ll = custom_loglike(beta_clipped, outcome_data_list)\n",
    "        numpyro.factor(f\"loglike_{s}\", ll)\n",
    "\n",
    "# -------------------------\n",
    "# Inference\n",
    "# -------------------------\n",
    "rng_key = random.PRNGKey(0)\n",
    "data_point_mean = data[\"point\"].mean()\n",
    "k = 5  # Number of mixture components\n",
    "\n",
    "# Set up and run MCMC using the NUTS sampler.\n",
    "nuts_kernel = NUTS(HDP_model)\n",
    "mcmc = MCMC(nuts_kernel, num_warmup=5000, num_samples=20000)\n",
    "mcmc.run(rng_key,\n",
    "         source_outcome_data=source_outcome_data,\n",
    "         num_outcomes_dict=num_outcomes_dict,\n",
    "         N_sources=N_sources,\n",
    "         k=k,\n",
    "         data_point_mean=data_point_mean)\n",
    "mcmc.print_summary()\n"
   ],
   "id": "e49b15f43190047",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|██████████| 25000/25000 [2:13:34<00:00,  3.12it/s, 1023 steps of size 2.99e-03. acc. prob=0.77]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                  mean       std    median      5.0%     95.0%     n_eff     r_hat\n",
      "      beta_1     11.62      9.14      9.36      1.59     23.66   1622.13      1.00\n",
      "      beta_2      4.67      7.46      1.62      0.67     16.86      7.99      1.14\n",
      "      beta_3      0.77      0.10      0.76      0.62      0.94   2499.92      1.00\n",
      "      beta_4      0.78      0.07      0.77      0.67      0.88   1066.27      1.00\n",
      "      beta_5      0.71      0.04      0.71      0.65      0.78   5192.40      1.00\n",
      "      beta_6      0.68      0.04      0.68      0.62      0.74   8503.28      1.00\n",
      "      beta_7      0.66      0.03      0.66      0.61      0.72   5393.66      1.00\n",
      "      beta_8      0.67      0.03      0.67      0.62      0.71   5681.14      1.00\n",
      "       mu[0]      2.41      8.12      0.75    -10.60     17.60    672.29      1.00\n",
      "       mu[1]      1.71      8.58      0.77    -12.62     17.75   1184.31      1.00\n",
      "       mu[2]      1.90      7.83      0.74    -11.32     16.21   1432.73      1.00\n",
      "       mu[3]      2.21      8.91      1.34    -12.33     17.37   2265.74      1.00\n",
      "       mu[4]      2.83      8.79      1.73    -11.73     18.11    690.38      1.01\n",
      "pi_global[0]      0.25      0.23      0.16      0.00      0.62     42.47      1.01\n",
      "pi_global[1]      0.19      0.20      0.11      0.00      0.54     48.49      1.02\n",
      "pi_global[2]      0.24      0.22      0.15      0.00      0.60     15.30      1.01\n",
      "pi_global[3]      0.16      0.17      0.10      0.00      0.40     54.37      1.01\n",
      "pi_global[4]      0.16      0.17      0.10      0.00      0.41     78.98      1.00\n",
      "    sigma[0]      5.37      6.02      3.37      0.02     14.10     91.64      1.00\n",
      "    sigma[1]      6.37      6.18      4.81      0.00     15.22    152.96      1.01\n",
      "    sigma[2]      5.27      5.94      3.36      0.00     13.96     38.67      1.01\n",
      "    sigma[3]      6.86      5.94      5.46      0.01     15.24    245.43      1.00\n",
      "    sigma[4]      6.86      6.07      5.54      0.00     15.41    332.15      1.00\n",
      "\n",
      "Number of divergences: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Custom Likelihood For Mean\n",
    "\n",
    "This notebook demonstrates a simple example to test whether a customized likelihood works. In this example, we:\n",
    "\n",
    "1. **Generate synthetic data:**  \n",
    "   We simulate 50 samples from a normal distribution, i.e., from \\$ N(5, 1) \\$. These data points represent our observed data.\n",
    "\n",
    "2. **Create a grid for \\$\\mu\\$ and compute its log-likelihood:**  \n",
    "   We create a grid of candidate \\$\\mu\\$ values (from 0 to 10) and, for each candidate, compute the total log-likelihood of the data (assuming known standard deviation \\$\\sigma=1\\$).  \n",
    "   The log-likelihood at each grid point is calculated as  \n",
    "   $$\n",
    "   \\text{loglike}(\\mu) = \\sum_{i=1}^{n} \\log \\left( \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{(y_i - \\mu)^2}{2}\\right) \\right)\n",
    "   $$\n",
    "   This grid serves as our precomputed likelihood surface.\n",
    "\n",
    "3. **Define a custom likelihood function:**  \n",
    "   The function `custom_loglike` uses linear interpolation (via JAX's `jnp.interp`) to \"look up\" the log-likelihood for any given value of \\$\\mu\\$, based on our precomputed grid.\n",
    "\n",
    "4. **Define the NumPyro model:**  \n",
    "   - We place a prior on \\$\\mu\\$ (a wide normal prior, e.g., \\$\\mu \\sim N(0, 10)\\$).\n",
    "   - We clip \\$\\mu\\$ to ensure it falls within the grid range.\n",
    "   - We incorporate the custom log-likelihood into the model using `numpyro.factor`, which adds this log probability to the joint log density.\n",
    "\n",
    "5. **Run inference using MCMC:**  \n",
    "   Finally, we run MCMC (with the NUTS sampler) to sample from the posterior of \\$\\mu\\$. If the model works correctly, the posterior should concentrate around the true value of 5.\n"
   ],
   "id": "e3937926f403945f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T17:39:15.546864Z",
     "start_time": "2025-03-24T17:39:11.965351Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "import numpy as np\n",
    "import numpyro\n",
    "import numpyro.distributions as dist\n",
    "from numpyro.infer import MCMC, NUTS\n",
    "\n",
    "# -------------------------\n",
    "# 1. Generate Synthetic Data\n",
    "# -------------------------\n",
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "# Generate 50 samples from N(5,1)\n",
    "y = np.random.normal(loc=5, scale=1, size=50)\n",
    "\n",
    "# -------------------------\n",
    "# 2. Create a Grid for μ and Compute its Log-Likelihood\n",
    "# -------------------------\n",
    "# Create a grid of μ values (from 0 to 10)\n",
    "grid_mu = np.linspace(0, 10, 100)\n",
    "# For each grid point, compute the total log likelihood of the data (assuming known sigma=1)\n",
    "from scipy.stats import norm\n",
    "loglike_grid = [np.sum(norm.logpdf(y, loc=mu, scale=1)) for mu in grid_mu]\n",
    "\n",
    "# Build a data array with columns: [mu (grid point), log-likelihood, outcome]\n",
    "# Since we only have one outcome here, set outcome = 1 for all rows.\n",
    "data_array = np.column_stack([grid_mu, loglike_grid, np.ones_like(grid_mu)])\n",
    "\n",
    "# -------------------------\n",
    "# 3. Define Helper Functions for the Custom Likelihood\n",
    "# -------------------------\n",
    "def interpolate(x0, y0, x):\n",
    "    \"\"\"\n",
    "    Linear interpolation using jnp.interp.\n",
    "    x0: grid points (assumed sorted)\n",
    "    y0: corresponding log-likelihood values\n",
    "    x: value at which to evaluate the interpolation\n",
    "    \"\"\"\n",
    "    return jnp.interp(x, x0, y0)\n",
    "\n",
    "def custom_loglike(mu, source_data):\n",
    "    \"\"\"\n",
    "    Custom log-likelihood function.\n",
    "    Here, source_data is expected to be a 2D array with columns [point, value, outcome].\n",
    "    Since we have only one outcome, we simply use all rows.\n",
    "    \"\"\"\n",
    "    # Extract the grid (points) and corresponding log-likelihood values.\n",
    "    x_vals = source_data[:, 0]\n",
    "    loglike_vals = source_data[:, 1]\n",
    "    # Interpolate to obtain the log-likelihood for the given mu.\n",
    "    return interpolate(x_vals, loglike_vals, mu)\n",
    "\n",
    "# -------------------------\n",
    "# 4. Define the NumPyro Model\n",
    "# -------------------------\n",
    "def model(source_data):\n",
    "    # Prior for μ: a wide Normal prior (centered at 0)\n",
    "    mu = numpyro.sample(\"mu\", dist.Normal(0, 10))\n",
    "    \n",
    "    # Optionally clip μ to be within the grid range\n",
    "    grid_min = jnp.min(source_data[:, 0])\n",
    "    grid_max = jnp.max(source_data[:, 0])\n",
    "    mu_clipped = jnp.clip(mu, grid_min, grid_max)\n",
    "    \n",
    "    # Compute the custom log-likelihood (interpolated from the grid)\n",
    "    ll = custom_loglike(mu_clipped, source_data)\n",
    "    # Add the custom log-likelihood to the model's joint log probability.\n",
    "    numpyro.factor(\"custom_ll\", ll)\n",
    "\n",
    "# -------------------------\n",
    "# 5. Run Inference with MCMC\n",
    "# -------------------------\n",
    "rng_key = random.PRNGKey(0)\n",
    "nuts_kernel = NUTS(model)\n",
    "mcmc = MCMC(nuts_kernel, num_warmup=1000, num_samples=2000)\n",
    "mcmc.run(rng_key, source_data=jnp.array(data_array))\n",
    "mcmc.print_summary()\n",
    "\n",
    "# Optionally, extract the posterior samples for μ:\n",
    "posterior_samples = mcmc.get_samples()[\"mu\"]\n",
    "print(\"Posterior mean for mu:\", np.mean(posterior_samples))\n"
   ],
   "id": "d3d12f60c246d60a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|██████████| 3000/3000 [00:01<00:00, 2816.62it/s, 3 steps of size 9.26e-01. acc. prob=0.91]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                mean       std    median      5.0%     95.0%     n_eff     r_hat\n",
      "        mu      4.77      0.14      4.78      4.53      5.00    765.25      1.00\n",
      "\n",
      "Number of divergences: 0\n",
      "Posterior mean for mu: 4.774934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "# Hierarchical Mixture Model with Customized Likelihood\n",
    "\n",
    "This notebook demonstrates a hierarchical mixture model using NumPyro that incorporates a custom likelihood function. The model is based on three fixed Gaussian components:\n",
    "\n",
    "- **Component 1:** $N(0,1)$  \n",
    "- **Component 2:** $N(4,1)$  \n",
    "- **Component 3:** $N(9,1)$  \n",
    "\n",
    "For each of multiple groups, the model assumes each group has its own mixing weights for these components, and these weights are modeled hierarchically using a Dirichlet distribution with a hyperprior on its concentration parameter.\n",
    "\n",
    "## Code Breakdown\n",
    "\n",
    "### 1. Synthetic Data Generation\n",
    "\n",
    "- **Purpose:**  \n",
    "  Generate synthetic data for three groups, with each group containing 50 observations.\n",
    "  \n",
    "- **Process:**  \n",
    "  - For each group, the “true” mixing weights are defined (these differ across groups).  \n",
    "  - For every observation in a group, a component is chosen based on these mixing weights, and a sample is drawn from the corresponding normal distribution.\n",
    "  \n",
    "- **Output:**  \n",
    "  The generated data for each group is stored in a list called `data_by_group`.\n",
    "\n",
    "### 2. Hierarchical Mixture Model Definition\n",
    "\n",
    "- **Hyperparameter $\\alpha_0$:**  \n",
    "  - Sampled from a Gamma distribution.  \n",
    "  - Acts as the concentration parameter for the Dirichlet distribution that models the group-specific mixing weights.\n",
    "\n",
    "- **Group-Specific Mixing Weights:**  \n",
    "  - For each group, mixing weights ($\\\\pi$) are sampled from a Dirichlet distribution parameterized by $\\\\alpha_0$ and a base measure (uniform in this case).\n",
    "\n",
    "- **Customized Likelihood:**  \n",
    "  - Instead of using a standard likelihood, the code defines a custom log-likelihood function for each group's data.\n",
    "  - For each observation $y$ in a group, the log probability is computed for each of the three components as:\n",
    "    - $\\log(\\\\pi_1) + \\log \\\\; N(y;0,1)$\n",
    "    - $\\log(\\\\pi_2) + \\log \\\\; N(y;4,1)$\n",
    "    - $\\log(\\\\pi_3) + \\log \\\\; N(y;9,1)$\n",
    "  - These values are combined using the log-sum-exp trick to compute the overall log likelihood for that observation.\n",
    "  - The total log-likelihood for all observations in a group is then added to the model’s joint log probability using `numpyro.factor`.\n",
    "\n",
    "### 3. Inference\n",
    "\n",
    "- **MCMC Sampling:**  \n",
    "  - The model uses the NUTS sampler to perform MCMC.\n",
    "  - The MCMC is configured with 1,000 warmup steps and 2,000 samples.\n",
    "  \n",
    "- **Outcome:**  \n",
    "  Posterior samples are drawn for the hyperparameter $\\\\alpha_0$ and the group-specific mixing weights $\\\\pi$.\n",
    "\n",
    "### 4. Posterior Analysis\n",
    "\n",
    "- **Post-Processing:**  \n",
    "  - After sampling, the code extracts the posterior samples for each group's mixing weights.\n",
    "  - It computes and prints the posterior mean for these weights, allowing a comparison with the true mixing weights used for data generation.\n",
    "\n",
    "## Summary\n",
    "\n",
    "- **Model Purpose:**  \n",
    "  Build a hierarchical Bayesian model for data from multiple groups, each modeled as a mixture of three fixed normal distributions.\n",
    "\n",
    "- **Customized Likelihood:**  \n",
    "  Instead of a standard likelihood, a custom likelihood is computed by manually combining the log probabilities from each mixture component. This likelihood is added as a factor in the model to contribute to the posterior.\n",
    "\n",
    "- **Inference:**  \n",
    "  MCMC via NUTS is used to sample from the posterior, and the resulting samples are used to analyze the inferred group-specific mixing weights.\n",
    "\n",
    "You can run the code cells in this notebook to see how the model fits the synthetic data and how well it recovers the true mixing weights.\n",
    "\n",
    "---"
   ],
   "id": "1389d2504bd321e5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T17:52:27.076091Z",
     "start_time": "2025-03-24T17:52:22.269245Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import jax.random as random\n",
    "import numpy as np\n",
    "import numpyro\n",
    "import numpyro.distributions as dist\n",
    "from numpyro.infer import MCMC, NUTS\n",
    "import jax.scipy.special\n",
    "\n",
    "# -------------------------\n",
    "# 1. Generate Synthetic Data\n",
    "# -------------------------\n",
    "# For reproducibility\n",
    "np.random.seed(0)\n",
    "n_groups = 3\n",
    "n_samples = 50\n",
    "\n",
    "# True mixing weights for each group (they differ across groups)\n",
    "true_mixing_weights = [\n",
    "    np.array([0.7, 0.2, 0.1]),\n",
    "    np.array([0.2, 0.5, 0.3]),\n",
    "    np.array([0.1, 0.2, 0.7])\n",
    "]\n",
    "\n",
    "# Fixed component parameters: N(0,1), N(4,1), N(9,1)\n",
    "components = [(0, 1), (4, 1), (9, 1)]\n",
    "data_by_group = []\n",
    "for g in range(n_groups):\n",
    "    group_data = []\n",
    "    for i in range(n_samples):\n",
    "        comp = np.random.choice(3, p=true_mixing_weights[g])\n",
    "        mu_true, sigma_true = components[comp]\n",
    "        y = np.random.normal(loc=mu_true, scale=sigma_true)\n",
    "        group_data.append(y)\n",
    "    data_by_group.append(np.array(group_data))\n",
    "\n",
    "# -------------------------\n",
    "# 2. Define the Hierarchical Mixture Model with Customized Likelihood\n",
    "# -------------------------\n",
    "def hierarchical_mixture_model(data_by_group):\n",
    "    # Hyperprior for the concentration parameter of the Dirichlet.\n",
    "    alpha0 = numpyro.sample(\"alpha0\", dist.Gamma(2.0, 1.0))\n",
    "    # A base measure (here uniform) for the Dirichlet distribution\n",
    "    base = jnp.ones(3)\n",
    "    \n",
    "    # For each group, sample group-specific mixing weights.\n",
    "    for g, data in enumerate(data_by_group):\n",
    "        pi = numpyro.sample(f\"pi_{g}\", dist.Dirichlet(alpha0 * base))\n",
    "        \n",
    "        # Define a custom log-likelihood for an individual observation y.\n",
    "        def mixture_loglik(y):\n",
    "            # For each component compute:\n",
    "            # log( pi_i ) + log( Normal(y; mu_i, sigma_i) )\n",
    "            log_comp1 = jnp.log(pi[0]) + dist.Normal(0, 1).log_prob(y)\n",
    "            log_comp2 = jnp.log(pi[1]) + dist.Normal(4, 1).log_prob(y)\n",
    "            log_comp3 = jnp.log(pi[2]) + dist.Normal(9, 1).log_prob(y)\n",
    "            # Combine the component contributions using logsumexp.\n",
    "            return jax.scipy.special.logsumexp(jnp.array([log_comp1, log_comp2, log_comp3]))\n",
    "        \n",
    "        # Evaluate the custom likelihood for all observations in the group.\n",
    "        group_loglik = jnp.sum(jax.vmap(mixture_loglik)(data))\n",
    "        # Add the custom log-likelihood factor to the joint log density.\n",
    "        numpyro.factor(f\"loglik_group_{g}\", group_loglik)\n",
    "\n",
    "# -------------------------\n",
    "# 3. Run Inference via MCMC\n",
    "# -------------------------\n",
    "rng_key = random.PRNGKey(0)\n",
    "nuts_kernel = NUTS(hierarchical_mixture_model)\n",
    "mcmc = MCMC(nuts_kernel, num_warmup=1000, num_samples=2000)\n",
    "mcmc.run(rng_key, data_by_group=data_by_group)\n",
    "mcmc.print_summary()\n",
    "\n",
    "# -------------------------\n",
    "# 4. Posterior Analysis\n",
    "# -------------------------\n",
    "# Extract and print the posterior means for the mixing weights for each group.\n",
    "posterior = mcmc.get_samples()\n",
    "for g in range(n_groups):\n",
    "    pi_samples = posterior[f\"pi_{g}\"]\n",
    "    print(f\"Group {g} mixing weights posterior mean:\", jnp.mean(pi_samples, axis=0))\n"
   ],
   "id": "47a11fa2c306e746",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|██████████| 3000/3000 [00:02<00:00, 1291.43it/s, 7 steps of size 6.92e-01. acc. prob=0.89]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                mean       std    median      5.0%     95.0%     n_eff     r_hat\n",
      "    alpha0      1.76      0.81      1.61      0.57      2.97   1942.62      1.00\n",
      "   pi_0[0]      0.71      0.06      0.72      0.61      0.81   2432.15      1.00\n",
      "   pi_0[1]      0.16      0.05      0.16      0.08      0.25   2741.87      1.00\n",
      "   pi_0[2]      0.12      0.05      0.12      0.05      0.19   2544.73      1.00\n",
      "   pi_1[0]      0.14      0.05      0.14      0.07      0.21   2466.89      1.00\n",
      "   pi_1[1]      0.57      0.07      0.57      0.45      0.67   2646.25      1.00\n",
      "   pi_1[2]      0.29      0.06      0.28      0.18      0.38   2859.15      1.00\n",
      "   pi_2[0]      0.14      0.05      0.14      0.06      0.22   3214.34      1.00\n",
      "   pi_2[1]      0.29      0.06      0.28      0.18      0.38   3428.17      1.00\n",
      "   pi_2[2]      0.57      0.07      0.58      0.46      0.68   3667.40      1.00\n",
      "\n",
      "Number of divergences: 0\n",
      "Group 0 mixing weights posterior mean: [0.71499145 0.1613238  0.12368481]\n",
      "Group 1 mixing weights posterior mean: [0.14350413 0.57071495 0.28578097]\n",
      "Group 2 mixing weights posterior mean: [0.14154607 0.28515053 0.57330346]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "928c0326a929261f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
